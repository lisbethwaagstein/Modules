{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exam: Scraping job postings from Jobindex\n",
    "\n",
    "...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scraping_class\n",
    "logfile = 'log_jobindex_scraping.txt' ## name your log file.\n",
    "connector = scraping_class.Connector(logfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "\n",
    "#define url and fetch the HTML using the requests module\n",
    "url = 'https://www.jobindex.dk/jobsoegning?lang=da.html'\n",
    "response = requests.get(url)  \n",
    "html = response.text  \n",
    "soup = BeautifulSoup(html,'lxml') # parse the raw html using BeautifoulSoup\n",
    "#print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "650\n"
     ]
    }
   ],
   "source": [
    "#find the max page number\n",
    "max_page = soup.find('ul',{'class':'pagination'})\n",
    "max_page2 = max_page.find_all('a', {'class':'page-link'})\n",
    "\n",
    "def convert_value_type(value_node):\n",
    "    if value_node.name == 'a':\n",
    "        return value_node.text\n",
    "\n",
    "page_list = []\n",
    "for page in max_page2:\n",
    "    page_list.append(convert_value_type(page))\n",
    "#print(page_list[-1])\n",
    "\n",
    "last_page = int(page_list[-1]) + 1 # we add one to use it in range in for-loop\n",
    "print(last_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all link pages from jobindex\n",
    "jobindex_links = []\n",
    "title_company = []\n",
    "job_data = []\n",
    "for i in range(1,last_page):\n",
    "    url = f'https://www.jobindex.dk/jobsoegning?lang=da?page={i}.html'\n",
    "    jobindex_links.append(url)   \n",
    "# Get html behind each search page from jobindex\n",
    "for link in jobindex_links:\n",
    "    response,call_id = connector.get(link,'searchpage')\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    joblistings = soup.find_all('div',{'class':'jobsearch-result'})\n",
    "# Extract info for each job listing from each searchpage    \n",
    "    for joblisting in joblistings:\n",
    "        # find the job title and company which are in bold\n",
    "        title_and_company = re.findall(r'(?<=<b>)(.*)(?=</b>)', str(joblisting)) \n",
    "        title_and_company = title_and_company[0:2]\n",
    "        soup = BeautifulSoup(str(joblisting), 'lxml')\n",
    "        # Indirectly extracting the short job description by removing everything else\n",
    "        for div in soup.find_all(\"div\", {'class':'jix_toolbar jix_appetizer_toolbar'}): # remove toolbar at the end of each job\n",
    "            div.decompose()\n",
    "        soup = soup.get_text()\n",
    "        soup = soup.replace(\"\\n\", \" \") # remove \\n \n",
    "        job_info = [str(title_and_company[0]),str(title_and_company[1]),str(soup)] #create a list containing job title, company and description\n",
    "        job_data.append(job_info)\n",
    "    \n",
    "#transforming into dataframe\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(job_data)\n",
    "df.columns = [\"Job Title\", \"Company\", \"Description\"]\n",
    "\n",
    "\n",
    "# remove job title and company info from description column\n",
    "df['Description'] = df['Description'].replace(to_replace=r'\\b'+df['Job Title']+r'\\b', value='',regex=True)\n",
    "df['Description'] = df['Description'].replace(to_replace=r'\\b'+df['Company']+r'\\b', value='',regex=True)\n",
    "df['Description'] = df['Description'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'url' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-3c793b7074fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpdfminer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhigh_level\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mextract_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'url' is not defined"
     ]
    }
   ],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "text = extract_text(url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
