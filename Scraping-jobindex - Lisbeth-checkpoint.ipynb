{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exam project: Scraping job postings from Jobindex\n",
    "\n",
    "This notebook is used for webscraping www.jobindex.dk for all job postings in the period xxxx-2020. "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
=======
   "execution_count": 226,
>>>>>>> db40826159e53a13837b9b8d5471eae784ab5213
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "#Make log\n",
    "import scraping_class\n",
    "logfile = 'log_jobindex_scraping.txt'\n",
    "connector = scraping_class.Connector(logfile)\n",
    "\n",
    "#Define todays date\n",
    "from datetime import date\n",
    "today = date.today().strftime(\"%Y%m%d\") # to get format 20080101"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the maximum page number to do a FOR loop through\n",
    "Each page of www.jobindex.dk/jobsoegning contains 20 joblistings. Since the number of job postings differ from time to time, so does the number of pages. To be able to scrape job postings on all pages, we look at the pagination. The pagination looks as the picture below, and we want to save the number of the last page as \"last_page\" to be able to loop through it later on.  \n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 14,
=======
   "execution_count": 532,
>>>>>>> db40826159e53a13837b9b8d5471eae784ab5213
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "637\n"
=======
      "616\n"
>>>>>>> db40826159e53a13837b9b8d5471eae784ab5213
     ]
    }
   ],
   "source": [
    "#Define the url and fetch the HTML using the requests module\n",
    "url = 'https://www.jobindex.dk/jobsoegning?lang=da.html'\n",
    "response = requests.get(url)  \n",
    "html = response.text  \n",
    "soup = BeautifulSoup(html,'lxml') # parse the raw html using BeautifoulSoup\n",
    "\n",
    "#Find the pagination and extract the text-part of the pagination links, i.e. not the link but the page number. \n",
    "pagination = soup.find_all('a', {'class':'page-link'})\n",
    "last_page = int(pagination[-1].text) #Save the last page number as \"last page\"\n",
    "print(last_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract all job postings from www.jobindex.dk. \n",
    "For each job posting one is redirected from www.jobindex.dk/jobsoegning to a separate www-webpage or PDF, that contains the full job posting. We therefore have to distinguish between these two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 0/40 of job_links\n",
      "Completed 1/40 of job_links\n",
      "Completed 2/40 of job_links\n",
      "Completed 3/40 of job_links\n",
      "Completed 4/40 of job_links\n",
      "Completed 5/40 of job_links\n",
      "Completed 6/40 of job_links\n",
      "Completed 7/40 of job_links\n",
      "Completed 8/40 of job_links\n",
      "Completed 9/40 of job_links\n",
      "Completed 10/40 of job_links\n",
      "Completed 11/40 of job_links\n",
      "Completed 12/40 of job_links\n",
      "Completed 13/40 of job_links\n",
      "Completed 14/40 of job_links\n",
      "Completed 15/40 of job_links\n",
      "Completed 16/40 of job_links\n",
      "Completed 17/40 of job_links\n",
      "Completed 18/40 of job_links\n",
      "Completed 19/40 of job_links\n",
      "Completed 20/40 of job_links\n",
      "Completed 21/40 of job_links\n",
      "Completed 22/40 of job_links\n",
      "Completed 23/40 of job_links\n",
      "Completed 24/40 of job_links\n",
      "Completed 25/40 of job_links\n",
      "Completed 26/40 of job_links\n",
      "Completed 27/40 of job_links\n",
      "Completed 28/40 of job_links\n",
      "Completed 29/40 of job_links\n",
      "Completed 30/40 of job_links\n",
      "Completed 31/40 of job_links\n",
      "Completed 32/40 of job_links\n",
      "Completed 33/40 of job_links\n",
      "Completed 34/40 of job_links\n",
      "Completed 35/40 of job_links\n",
      "Completed 36/40 of job_links\n",
      "Completed 37/40 of job_links\n",
      "Completed 38/40 of job_links\n",
      "Completed 39/40 of job_links\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2 \n",
    "import textract\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import slate3k as slate\n",
    "import pandas as pd\n",
    "\n",
    "jobindex_links = []\n",
    "for i in range(1,3): # NOTE I'm only testing it on the first two pages. Use last_page when ready\n",
    "    url = f'https://www.jobindex.dk/jobsoegning?lang=da?page={i}.html'\n",
    "    jobindex_links.append(url)   \n",
    "#print(jobindex_links)\n",
    "html = []\n",
    "job_links = []\n",
    "jobs = []\n",
    "dates = []\n",
    "\n",
    "for url in jobindex_links:\n",
    "    response = requests.get(url)  \n",
    "    html = response.text  \n",
    "    soup = BeautifulSoup(html,'lxml')\n",
    "    #one job result is given by class=jobsearch-result\n",
    "    joblistings = soup.find_all('div',{'class':'jobsearch-result'})\n",
    "    #print(joblistings)\n",
    "    for joblisting in joblistings:\n",
    "        title = re.findall(r'(?<=<b>)(.*)(?=</b>)', str(joblisting))[0]\n",
    "        links = re.findall(\"href=[\\\"\\'](.*?)[\\\"\\']\", str(joblisting))\n",
    "        link = str(links[1])\n",
    "        link = link.replace(\"&amp;\", \"&\")\n",
    "        pub_date = re.findall(r'time\\sdatetime=\"(.*)\"', str(joblisting))\n",
    "        dates.append(pub_date) \n",
    "        job = [title,link,pub_date]\n",
    "        jobs.append(job) # Jobs is a list of list where each list contains the title job, and the link for that job, this will help later\n",
    "        job_links.append(link)\n",
    "\n",
    "for i in range(len(job_links)):\n",
    "    if 'pdf' in jobs[i][1]:\n",
    "        #scraping a PDF file\n",
    "        #request the url\n",
    "        url = jobs[i][1]\n",
    "        response = requests.get(url)\n",
    "        #save the pdf in the current folder\n",
    "        with open('pdf.pdf', 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        #open the pdf and get the text in doc\n",
    "        with open('pdf.pdf', 'rb') as fp:\n",
    "            doc = slate.PDF(fp)\n",
    "            doc = str(doc).replace('\\n','')\n",
    "            doc = str(doc).replace('\\r','')\n",
    "            jobs[i].append(doc)\n",
    "    else:\n",
    "        link = jobs[i][1]\n",
    "        response = requests.get(link)\n",
    "        html = response.text\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        description = soup.get_text()\n",
    "        description = description.replace('\\n','')\n",
    "        description = description.replace('\\r','')\n",
    "        jobs[i].append(description)\n",
    "    \n",
    "    print('Completed %d/%d job_links' % (i,len(job_links)))\n",
    "\n",
    "#transforming into dataframe\n",
    "\n",
    "df = pd.DataFrame(jobs)\n",
    "df.columns = [\"Job Title\", \"Link\",\"Date\", \"Description\",]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying jobs with possibility of working remotely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we define a list of keyword that we think a job posting where working remotely is a possibility will include\n",
    "\n",
    "keywords = ['hjemmefra', 'arbejde hjemme', 'hjemmearbejde','hjemmekontor', 'arbejde hjemmefra', 'arbejde remote', 'fjernarbejde']\n",
    "\n",
    "# I will now look for these words in the description column. I am not using the tokenized version of column because then I \n",
    "# can't look for expressions with more than one word. If you want to use the tokenized version then you have to use bigrams\n",
    "\n",
    "df['Remote'] = '0'\n",
    "for word in keywords:\n",
    "    for i in range(len(df)):\n",
    "        df['Description'][i] = str(df['Description'][i]).lower()\n",
    "        if word in df['Description'][i]:\n",
    "            df['Remote'][i] = '1'\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying the most popular words in job descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first step is to clean the job descriptions as much as possible so we can narrow down the words included for \n",
    "# analysis as much as possible\n",
    "\n",
    "df['Tokenized_description'] = ' '\n",
    "symbols = ['?','!','>','<','-','[',']','(',')','{','}',' –','``',\"''\",'\"\"','\\\\','@','$','&','=']\n",
    "for i in range(len(df)):\n",
    "    stop_words = nltk.corpus.stopwords.words(\"danish\")\n",
    "    df['Tokenized_description'][i] = re.sub(r'(\\.+ )|,|\\||:|/|\\'|\\-|;|\\*|!|(\\s\\d+\\s)|(\\s\\W\\s)',' ',str(df['Description'][i]))\n",
    "    df['Tokenized_description'][i] = str(df['Tokenized_description'][i]).rstrip('\\\\')\n",
    "    df['Tokenized_description'][i] = nltk.word_tokenize(str(df['Tokenized_description'][i].lower()))\n",
    "    df['Tokenized_description'][i] = [w for w in df['Tokenized_description'][i] if not w in stop_words] \n",
    "    df['Tokenized_description'][i] = [w for w in df['Tokenized_description'][i] if not w in symbols]\n",
    "    df['Tokenized_description'][i] = [w for w in df['Tokenized_description'][i] if not w.isnumeric()]\n",
    "    df['Tokenized_description'][i] = [word for word in df['Tokenized_description'][i] if len(word) > 3]\n",
    "    \n",
    "# Finding intersections --- USELESS\n",
    "df['description_set'] = df['Tokenized_description'].apply(set)\n",
    "description_set = df['description_set'].tolist() #convert each description into a set \n",
    "set.intersection(*description_set)\n",
    "del df['description_set']\n",
    "\n",
    "# Finding most frequent words in all descriptions --- USELESS ATM BUT MAYBE IF WE CLEAN IT ENOUGH IT WILL WORK\n",
    "\n",
    "descriptions_list = [] \n",
    "# This loop will pull all tokens in one bag\n",
    "for i in range(len(df)):\n",
    "    descriptions_list.extend(df['Tokenized_description'][i]) \n",
    "    \n",
    "word_dist = nltk.FreqDist(descriptions_list) word_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL DEDICATED TO KEEP CLEANING DESCRIPTIONS\n",
    "\n",
    "def clean(doc):\n",
    "    #doc = doc.replace('\\n','')\n",
    "    #doc = doc.replace('\\r','')\n",
    "    doc = doc.replace('\\t','')\n",
    "    doc = doc.replace('\\'','')\n",
    "    doc = doc.replace('|','')\n",
    "    doc = doc.replace('/','')\n",
    "\n",
    "def strip_html(row):\n",
    "    return str(html.fromstring(row).text_content())\n",
    "\n",
    "from lxml import html\n",
    "for row in df['Description']:\n",
    "    strip_html(str(row))\n",
    "    \n",
    "    \n",
    "    \n",
    "#text = re.sub('<[^>]*>', '', text)\n",
    "#emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assigning DISCO-08 codes to the job postings\n",
    "This function will be used to assign a DISCO-08 code to the job postings. It goes through the following steps:\n",
    "\n",
    "1. Importing a csv file downloaded from Danmarks Statistik which includes the different job functions that fall under each DISCO-08 category\n",
    "2. Clean this csv file and converting it into a dataframe with two columns, one that has the disco codes and one that has a list of all the jobs that fall into each code\n",
    "3. Tokenizing the jobs \n",
    "4. Tokenizing the words in the 'Job Title' column from our job postings data\n",
    "5. Creating a function that compares the tokens in the job titles to the tokens in the DISCO dataframe and if a match is found, then returning the corresponding DISCO code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lisbe\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "# Importing and cleaning DISCO-08 classification\n",
    "import pandas as pd\n",
    "import re\n",
    "disco = pd.read_csv(r\"C:\\Users\\lisbe\\OneDrive\\11. Semester\\Social Data Science\\Modules\\csv_da.csv\",header=None)\n",
    "disco_clean = disco.copy()\n",
    "\n",
    "# Remove words that start with lowercase\n",
    "for i in range(len(disco_clean)):\n",
    "    disco_clean[3][i] = ' '.join([word for word in str(disco_clean[3][i]).split(' ') if not word.islower()])\n",
    "\n",
    "# Create a dictionary for DISCO functions\n",
    "\n",
    "disco_clean['DISCO'] = disco_clean[0].astype(str).str[:1] #This column will have the highest hierarchy code 0-9\n",
    "del disco_clean[0]\n",
    "del disco_clean[1]\n",
    "del disco_clean[2]\n",
    "disco_clean = disco_clean.groupby(by=disco_clean['DISCO']).sum()\n",
    "disco_clean.rename({3:'functions'}, axis='columns', inplace = True) # rename column \n",
    "for i in range(9): # Adding a space in between words that are missing them \"LikeThis\"\n",
    "    disco_clean['DISCO'] = disco_clean['DISCO'].astype(str).str[:1] # We only want the first number \n",
    "    disco_clean['functions'][i] = re.sub(r\"(\\w)([A-Z])\", r\"\\1 \\2\", str(disco_clean['functions'][i]))\n",
    "    \n",
    "disco_clean[\"functions_tokenized\"] = \"\"\n",
    "for i in range(len(disco_clean)):\n",
    "    disco_clean['functions_tokenized'][i]=nltk.word_tokenize(str(disco_clean['functions'][i]).lower())\n",
    "    \n",
    "# Now I would like a job type column from the job titles where I would extract the nouns from job titles\n",
    "#pip install afinn\n",
    "import nltk\n",
    "df['Job Title']=df['Job Title'].str.replace(',','')\n",
    "df[\"tokenized_titles\"] = \"\"\n",
    "for i in range(len(df)):\n",
    "    df['tokenized_titles'][i] = nltk.word_tokenize(str(df['Job Title'][i]).lower())\n",
    "\n",
    "# This loop creates a new column in the df that assigns a disco code to each posting based on the words in the title\n",
    "\n",
    "df['disco'] = ''\n",
    "for m in range(len(df)):\n",
    "    for i in range(len(disco_clean)):\n",
    "        for element in disco_clean['functions_tokenized'][i]:\n",
    "            if element in df['tokenized_titles'][m]:\n",
    "                df['disco'][m] = disco_clean['DISCO'][i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape the joblistings for each geographical area\n",
    "www.jobindex.dk/jobsoegning has a filter for which geographical area, the job posting is located in. We will scrape this to be able to describe geographical differences in where remote working is taking place. The address of the company is also attached to the job posting, and this information would be more detailed. However, we have experienced that the address often belongs to the headquarter of the company and not the specific area, where the job is. Therefore we use the information from the geographical filter.\n",
    "We will loop through the area filter and extract job titles and company, which can then be merged onto the big dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "area: nordsjaelland\n",
      "area: storkoebenhavn\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-e0a9931aec1e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf'https://www.jobindex.dk/jobsoegning/{area}?jobage=archive&maxdate={today}&mindate=20080101&page={page}'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[0mhtml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[0mhtml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'allow_redirects'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'get'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    528\u001b[0m         }\n\u001b[0;32m    529\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    531\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    641\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    642\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 643\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    644\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m                 resp = conn.urlopen(\n\u001b[0m\u001b[0;32m    440\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m                     \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    668\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m             \u001b[1;31m# Make the request on the httplib connection object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 670\u001b[1;33m             httplib_response = self._make_request(\n\u001b[0m\u001b[0;32m    671\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    424\u001b[0m                     \u001b[1;31m# Python 3 (including for exceptions like SystemExit).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m                     \u001b[1;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 426\u001b[1;33m                     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    427\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    419\u001b[0m                 \u001b[1;31m# Python 3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 421\u001b[1;33m                     \u001b[0mhttplib_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    422\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m                     \u001b[1;31m# Remove the TypeError from the exception chain in\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1330\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1331\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1332\u001b[1;33m                 \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1333\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1334\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mbegin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    301\u001b[0m         \u001b[1;31m# read until we get a non-100 response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 303\u001b[1;33m             \u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    304\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 264\u001b[1;33m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"iso-8859-1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    265\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"status line\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    667\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 669\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    670\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1239\u001b[0m                   \u001b[1;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1240\u001b[0m                   self.__class__)\n\u001b[1;32m-> 1241\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1242\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1243\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1097\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1099\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1100\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1101\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import re\n",
    "import requests\n",
    "#Define all the urls for the different geographical areas\n",
    "url = 'https://www.jobindex.dk/jobsoegning'\n",
    "response = requests.get(url)\n",
    "html = response.text\n",
    "soup = BeautifulSoup(html,'lxml')\n",
    "\n",
    "#Areas are given in the html by div id=areas and class=area_label\n",
    "areas_html = soup.find('div', {'id':'areas'})#('a', {'class':'area_label'})\n",
    "areas_div = str(areas_html).split('href=\"/jobsoegning/')[1:]\n",
    "\n",
    "#Find the links to each of the areas\n",
    "areas = set()\n",
    "areas_delete = ('skaane', 'faeroeerne', 'udlandet', 'groenland') #delete regions outside Denmark's borders\n",
    "for area in areas_div:\n",
    "    link = area.split('\"')[0]\n",
    "    if link in areas_delete:\n",
    "        del link\n",
    "    else:\n",
    "        areas.add(link)\n",
    "#print(areas)\n",
    " \n",
    "#Scraping the geographical areas from jobindex.\n",
    "area_jobs = []\n",
    "\n",
    "from datetime import date\n",
    "today = date.today().strftime(\"%Y%m%d\") # to get format 20080101\n",
    "\n",
    "# first loop through areas\n",
    "for area in areas:\n",
    "    print('area: ' + area)\n",
    "    url = f'https://www.jobindex.dk/jobsoegning/{area}?jobage=archive&maxdate={today}&mindate=20080101'\n",
    "    response = requests.get(url)  \n",
    "    html = response.text  \n",
    "    soup = BeautifulSoup(html,'lxml')   \n",
    "    \n",
    "    pagination = soup.find_all('a', {'class':'page-link'})\n",
    "    if len(pagination) == 0:\n",
    "        continue\n",
    "    last_page = int(pagination[-1].text) \n",
    "    \n",
    "    # loop through pages\n",
    "    for page in range(1,last_page+1):\n",
    "        if  page % 25 == 0:\n",
    "            print('page: ',page,'/', last_page+1)\n",
    "            \n",
    "        url = f'https://www.jobindex.dk/jobsoegning/{area}?jobage=archive&maxdate={today}&mindate=20080101&page={page}'\n",
    "        response = requests.get(url)  \n",
    "        html = response.text  \n",
    "        html = response.text  \n",
    "        if '<strong>' in html: \n",
    "            html = html.replace('<strong>','<b>')\\\n",
    "                   .replace('</strong>','</b>')\n",
    "        else:\n",
    "            print('No <strong>')\n",
    "        soup = BeautifulSoup(html,'lxml') \n",
    "    \n",
    "    #loop through job listings\n",
    "        joblistings = soup.find_all('div',{'class':'jobsearch-result'}) \n",
    "        for joblisting in joblistings:\n",
    "            title = re.findall(r'(?<=<b>)(.*)(?=</b>)', str(joblisting))[0]\n",
    "            if len(re.findall(r'(?<=<b>)(.*)(?=</b>)', str(joblisting)))>1:\n",
    "                company = re.findall(r'(?<=<b>)(.*)(?=</b>)', str(joblisting))[1]\n",
    "            else:\n",
    "                company = ''\n",
    "            geo_area = area\n",
    "            job = [title, company, geo_area]\n",
    "            area_jobs.append(job)\n",
    "        \n",
    "#Make into dataframe\n",
    "import pandas as pd\n",
    "area_data = pd.DataFrame(area_jobs)\n",
    "area_data.columns = [\"Job Title\", \"Company\", \"Geographical Area\"]\n",
    "area_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Geographical Area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Customer Support Specialist - Sweden</td>\n",
       "      <td>SameSystem A/S</td>\n",
       "      <td>nordsjaelland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Klinikassistent</td>\n",
       "      <td>Fredensborg Kommune&lt;/b&gt;, &lt;b&gt;Humlebæk</td>\n",
       "      <td>nordsjaelland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lærer</td>\n",
       "      <td>Fredensborg Skole&lt;/b&gt;, &lt;b&gt;Fredensborg</td>\n",
       "      <td>nordsjaelland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vuggestuepædagog</td>\n",
       "      <td>Paddehatten Vuggestue&lt;/b&gt;, &lt;b&gt;Fredensborg</td>\n",
       "      <td>nordsjaelland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Senior Procurement Manager, Indirect Procurement</td>\n",
       "      <td>Bang &amp;amp; Olufsen A/S</td>\n",
       "      <td>nordsjaelland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>Frivillige søges til vores Eventteam</td>\n",
       "      <td>Røde Kors Hovedstaden&lt;/b&gt;, &lt;b&gt;København N</td>\n",
       "      <td>storkoebenhavn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>Talstærk studerende med interesse for økonomi ...</td>\n",
       "      <td>Økonomi og Stab&lt;/b&gt;, &lt;b&gt;Albertslund</td>\n",
       "      <td>storkoebenhavn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>Handicaphjælper</td>\n",
       "      <td>København Ø</td>\n",
       "      <td>storkoebenhavn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>Handicaphjælper</td>\n",
       "      <td>Herlev</td>\n",
       "      <td>storkoebenhavn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>Handicaphjælper</td>\n",
       "      <td>København Ø</td>\n",
       "      <td>storkoebenhavn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Job Title  \\\n",
       "0                 Customer Support Specialist - Sweden   \n",
       "1                                      Klinikassistent   \n",
       "2                                                Lærer   \n",
       "3                                     Vuggestuepædagog   \n",
       "4     Senior Procurement Manager, Indirect Procurement   \n",
       "..                                                 ...   \n",
       "235               Frivillige søges til vores Eventteam   \n",
       "236  Talstærk studerende med interesse for økonomi ...   \n",
       "237                                    Handicaphjælper   \n",
       "238                                    Handicaphjælper   \n",
       "239                                    Handicaphjælper   \n",
       "\n",
       "                                       Company Geographical Area  \n",
       "0                               SameSystem A/S     nordsjaelland  \n",
       "1         Fredensborg Kommune</b>, <b>Humlebæk     nordsjaelland  \n",
       "2        Fredensborg Skole</b>, <b>Fredensborg     nordsjaelland  \n",
       "3    Paddehatten Vuggestue</b>, <b>Fredensborg     nordsjaelland  \n",
       "4                       Bang &amp; Olufsen A/S     nordsjaelland  \n",
       "..                                         ...               ...  \n",
       "235  Røde Kors Hovedstaden</b>, <b>København N    storkoebenhavn  \n",
       "236        Økonomi og Stab</b>, <b>Albertslund    storkoebenhavn  \n",
       "237                                København Ø    storkoebenhavn  \n",
       "238                                     Herlev    storkoebenhavn  \n",
       "239                                København Ø    storkoebenhavn  \n",
       "\n",
       "[240 rows x 3 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Make into dataframe\n",
    "import pandas as pd\n",
    "area_data = pd.DataFrame(area_jobs)\n",
    "area_data.columns = [\"Job Title\", \"Company\", \"Geographical Area\"]\n",
    "area_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
<<<<<<< HEAD
=======
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
>>>>>>> db40826159e53a13837b9b8d5471eae784ab5213
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
