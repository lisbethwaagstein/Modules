{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exam project: Scraping job postings from Jobindex\n",
    "\n",
    "This notebook is used for webscraping www.jobindex.dk for all job postings in the period xxxx-2020. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "#Make log\n",
    "import scraping_class\n",
    "logfile = 'log_jobindex_scraping.txt'\n",
    "connector = scraping_class.Connector(logfile)\n",
    "\n",
    "#Define todays date\n",
    "from datetime import date\n",
    "today = date.today().strftime(\"%Y%m%d\") # to get format 20080101"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the maximum page number to do a FOR loop through\n",
    "Each page of www.jobindex.dk/jobsoegning contains 20 joblistings. Since the number of job postings differ from time to time, so does the number of pages. To be able to scrape job postings on all pages, we look at the pagination. The pagination looks as the picture below, and we want to save the number of the last page as \"last_page\" to be able to loop through it later on.  \n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "650\n"
     ]
    }
   ],
   "source": [
    "#Define the url and fetch the HTML using the requests module\n",
    "url = 'https://www.jobindex.dk/jobsoegning?lang=da.html'\n",
    "response = requests.get(url)  \n",
    "html = response.text  \n",
    "soup = BeautifulSoup(html,'lxml') # parse the raw html using BeautifoulSoup\n",
    "\n",
    "#Find the pagination and extract the text-part of the pagination links, i.e. not the link but the page number. \n",
    "pagination = soup.find_all('a', {'class':'page-link'})\n",
    "last_page = int(pagination[-1].text) #Save the last page number as \"last page\"\n",
    "print(last_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract all job postings from www.jobindex.dk. \n",
    "For each job posting one is redirected from www.jobindex.dk/jobsoegning to a separate www-webpage or PDF, that contains the full job posting. We therefore have to distinguish between these two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 0/40 of job_links\n",
      "Completed 1/40 of job_links\n",
      "Completed 2/40 of job_links\n",
      "Completed 3/40 of job_links\n",
      "Completed 4/40 of job_links\n",
      "Completed 5/40 of job_links\n",
      "Completed 6/40 of job_links\n",
      "Completed 7/40 of job_links\n",
      "Completed 8/40 of job_links\n",
      "Completed 9/40 of job_links\n",
      "Completed 10/40 of job_links\n",
      "Completed 11/40 of job_links\n",
      "Completed 12/40 of job_links\n",
      "Completed 13/40 of job_links\n",
      "Completed 14/40 of job_links\n",
      "Completed 15/40 of job_links\n",
      "Completed 16/40 of job_links\n",
      "Completed 17/40 of job_links\n",
      "Completed 18/40 of job_links\n",
      "Completed 19/40 of job_links\n",
      "Completed 20/40 of job_links\n",
      "Completed 21/40 of job_links\n",
      "Completed 22/40 of job_links\n",
      "Completed 23/40 of job_links\n",
      "Completed 24/40 of job_links\n",
      "Completed 25/40 of job_links\n",
      "Completed 26/40 of job_links\n",
      "Completed 27/40 of job_links\n",
      "Completed 28/40 of job_links\n",
      "Completed 29/40 of job_links\n",
      "Completed 30/40 of job_links\n",
      "Completed 31/40 of job_links\n",
      "Completed 32/40 of job_links\n",
      "Completed 33/40 of job_links\n",
      "Completed 34/40 of job_links\n",
      "Completed 35/40 of job_links\n",
      "Completed 36/40 of job_links\n",
      "Completed 37/40 of job_links\n",
      "Completed 38/40 of job_links\n",
      "Completed 39/40 of job_links\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2 \n",
    "import textract\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import slate3k as slate\n",
    "import pandas as pd\n",
    "\n",
    "jobindex_links = []\n",
    "for i in range(1,3): # NOTE I'm only testing it on the first two pages. Use last_page when ready\n",
    "    url = f'https://www.jobindex.dk/jobsoegning?lang=da?page={i}.html'\n",
    "    jobindex_links.append(url)   \n",
    "#print(jobindex_links)\n",
    "html = []\n",
    "job_links = []\n",
    "jobs = []\n",
    "dates = []\n",
    "\n",
    "for url in jobindex_links:\n",
    "    response = requests.get(url)  \n",
    "    html = response.text  \n",
    "    soup = BeautifulSoup(html,'lxml')\n",
    "    #one job result is given by class=jobsearch-result\n",
    "    joblistings = soup.find_all('div',{'class':'jobsearch-result'})\n",
    "    #print(joblistings)\n",
    "    for joblisting in joblistings:\n",
    "        title = re.findall(r'(?<=<b>)(.*)(?=</b>)', str(joblisting))[0]\n",
    "        links = re.findall(\"href=[\\\"\\'](.*?)[\\\"\\']\", str(joblisting))\n",
    "        link = str(links[1])\n",
    "        link = link.replace(\"&amp;\", \"&\")\n",
    "        pub_date = re.findall(r'time\\sdatetime=\"(.*)\"', str(joblisting))\n",
    "        dates.append(pub_date) \n",
    "        job = [title,link,pub_date]\n",
    "        jobs.append(job) # Jobs is a list of list where each list contains the title job, and the link for that job, this will help later\n",
    "        job_links.append(link)\n",
    "\n",
    "for i in range(len(job_links)):\n",
    "    if 'pdf' in jobs[i][1]:\n",
    "        #scraping a PDF file\n",
    "        #request the url\n",
    "        url = jobs[i][1]\n",
    "        response = requests.get(url)\n",
    "        #save the pdf in the current folder\n",
    "        with open('pdf.pdf', 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        #open the pdf and get the text in doc\n",
    "        with open('pdf.pdf', 'rb') as fp:\n",
    "            doc = slate.PDF(fp)\n",
    "            doc = str(doc).replace('\\n','')\n",
    "            doc = str(doc).replace('\\r','')\n",
    "            jobs[i].append(doc)\n",
    "    else:\n",
    "        link = jobs[i][1]\n",
    "        response = requests.get(link)\n",
    "        html = response.text\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        description = soup.get_text()\n",
    "        description = description.replace('\\n','')\n",
    "        description = description.replace('\\r','')\n",
    "        jobs[i].append(description)\n",
    "    \n",
    "    print('Completed %d/%d job_links' % (i,len(job_links)))\n",
    "\n",
    "#transforming into dataframe\n",
    "\n",
    "df = pd.DataFrame(jobs)\n",
    "df.columns = [\"Job Title\", \"Link\",\"Date\", \"Description\",]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying jobs with possibility of working remotely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we define a list of keyword that we think a job posting where working remotely is a possibility will include\n",
    "\n",
    "keywords = ['hjemmefra', 'arbejde hjemme', 'hjemmearbejde','hjemmekontor', 'arbejde hjemmefra', 'arbejde remote', 'fjernarbejde']\n",
    "\n",
    "# I will now look for these words in the description column. I am not using the tokenized version of column because then I \n",
    "# can't look for expressions with more than one word. If you want to use the tokenized version then you have to use bigrams\n",
    "\n",
    "df['Remote'] = '0'\n",
    "for word in keywords:\n",
    "    for i in range(len(df)):\n",
    "        df['Description'][i] = str(df['Description'][i]).lower()\n",
    "        if word in df['Description'][i]:\n",
    "            df['Remote'][i] = '1'\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying the most popular words in job descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first step is to clean the job descriptions as much as possible so we can narrow down the words included for \n",
    "# analysis as much as possible\n",
    "\n",
    "df['Tokenized_description'] = ' '\n",
    "symbols = ['?','!','>','<','-','[',']','(',')','{','}',' –','``',\"''\",'\"\"','\\\\','@','$','&','=']\n",
    "for i in range(len(df)):\n",
    "    stop_words = nltk.corpus.stopwords.words(\"danish\")\n",
    "    df['Tokenized_description'][i] = re.sub(r'(\\.+ )|,|\\||:|/|\\'|\\-|;|\\*|!|(\\s\\d+\\s)|(\\s\\W\\s)',' ',str(df['Description'][i]))\n",
    "    df['Tokenized_description'][i] = str(df['Tokenized_description'][i]).rstrip('\\\\')\n",
    "    df['Tokenized_description'][i] = nltk.word_tokenize(str(df['Tokenized_description'][i].lower()))\n",
    "    df['Tokenized_description'][i] = [w for w in df['Tokenized_description'][i] if not w in stop_words] \n",
    "    df['Tokenized_description'][i] = [w for w in df['Tokenized_description'][i] if not w in symbols]\n",
    "    df['Tokenized_description'][i] = [w for w in df['Tokenized_description'][i] if not w.isnumeric()]\n",
    "    df['Tokenized_description'][i] = [word for word in df['Tokenized_description'][i] if len(word) > 3]\n",
    "    \n",
    "# Finding intersections --- USELESS\n",
    "df['description_set'] = df['Tokenized_description'].apply(set)\n",
    "description_set = df['description_set'].tolist() #convert each description into a set \n",
    "set.intersection(*description_set)\n",
    "del df['description_set']\n",
    "\n",
    "# Finding most frequent words in all descriptions --- USELESS ATM BUT MAYBE IF WE CLEAN IT ENOUGH IT WILL WORK\n",
    "\n",
    "descriptions_list = [] \n",
    "# This loop will pull all tokens in one bag\n",
    "for i in range(len(df)):\n",
    "    descriptions_list.extend(df['Tokenized_description'][i]) \n",
    "    \n",
    "word_dist = nltk.FreqDist(descriptions_list) word_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL DEDICATED TO KEEP CLEANING DESCRIPTIONS\n",
    "\n",
    "def clean(doc):\n",
    "    #doc = doc.replace('\\n','')\n",
    "    #doc = doc.replace('\\r','')\n",
    "    doc = doc.replace('\\t','')\n",
    "    doc = doc.replace('\\'','')\n",
    "    doc = doc.replace('|','')\n",
    "    doc = doc.replace('/','')\n",
    "\n",
    "def strip_html(row):\n",
    "    return str(html.fromstring(row).text_content())\n",
    "\n",
    "from lxml import html\n",
    "for row in df['Description']:\n",
    "    strip_html(str(row))\n",
    "    \n",
    "    \n",
    "    \n",
    "#text = re.sub('<[^>]*>', '', text)\n",
    "#emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assigning DISCO-08 codes to the job postings\n",
    "This function will be used to assign a DISCO-08 code to the job postings. It goes through the following steps:\n",
    "\n",
    "1. Importing a csv file downloaded from Danmarks Statistik which includes the different job functions that fall under each DISCO-08 category\n",
    "2. Clean this csv file and converting it into a dataframe with two columns, one that has the disco codes and one that has a list of all the jobs that fall into each code\n",
    "3. Tokenizing the jobs \n",
    "4. Tokenizing the words in the 'Job Title' column from our job postings data\n",
    "5. Creating a function that compares the tokens in the job titles to the tokens in the DISCO dataframe and if a match is found, then returning the corresponding DISCO code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lisbe\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "# Importing and cleaning DISCO-08 classification\n",
    "import pandas as pd\n",
    "import re\n",
    "disco = pd.read_csv(r\"C:\\Users\\lisbe\\OneDrive\\11. Semester\\Social Data Science\\Modules\\csv_da.csv\",header=None)\n",
    "disco_clean = disco.copy()\n",
    "\n",
    "# Remove words that start with lowercase\n",
    "for i in range(len(disco_clean)):\n",
    "    disco_clean[3][i] = ' '.join([word for word in str(disco_clean[3][i]).split(' ') if not word.islower()])\n",
    "\n",
    "# Create a dictionary for DISCO functions\n",
    "\n",
    "disco_clean['DISCO'] = disco_clean[0].astype(str).str[:1] #This column will have the highest hierarchy code 0-9\n",
    "del disco_clean[0]\n",
    "del disco_clean[1]\n",
    "del disco_clean[2]\n",
    "disco_clean = disco_clean.groupby(by=disco_clean['DISCO']).sum()\n",
    "disco_clean.rename({3:'functions'}, axis='columns', inplace = True) # rename column \n",
    "for i in range(9): # Adding a space in between words that are missing them \"LikeThis\"\n",
    "    disco_clean['DISCO'] = disco_clean['DISCO'].astype(str).str[:1] # We only want the first number \n",
    "    disco_clean['functions'][i] = re.sub(r\"(\\w)([A-Z])\", r\"\\1 \\2\", str(disco_clean['functions'][i]))\n",
    "    \n",
    "disco_clean[\"functions_tokenized\"] = \"\"\n",
    "for i in range(len(disco_clean)):\n",
    "    disco_clean['functions_tokenized'][i]=nltk.word_tokenize(str(disco_clean['functions'][i]).lower())\n",
    "    \n",
    "# Now I would like a job type column from the job titles where I would extract the nouns from job titles\n",
    "#pip install afinn\n",
    "import nltk\n",
    "df['Job Title']=df['Job Title'].str.replace(',','')\n",
    "df[\"tokenized_titles\"] = \"\"\n",
    "for i in range(len(df)):\n",
    "    df['tokenized_titles'][i] = nltk.word_tokenize(str(df['Job Title'][i]).lower())\n",
    "\n",
    "# This loop creates a new column in the df that assigns a disco code to each posting based on the words in the title\n",
    "\n",
    "df['disco'] = ''\n",
    "for m in range(len(df)):\n",
    "    for i in range(len(disco_clean)):\n",
    "        for element in disco_clean['functions_tokenized'][i]:\n",
    "            if element in df['tokenized_titles'][m]:\n",
    "                df['disco'][m] = disco_clean['DISCO'][i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape the joblistings for each geographical area and job categories/industries\n",
    "www.jobindex.dk/jobsoegning has a filter for geographical area and job category, respectively. These can be seen in the picture below. We will scrape this to be able to describe in which areas and industries remote working is especially taking place. The address of the company is also attached to the job posting, and even though this information would be more detailed, we have experienced that the address often belongs to the headquarter of the company and not the specific area, where the job is. Therefore we use the information from the geographical filter.\n",
    "We will loop through the filters and extract job titles and company, which can then be merged onto the big dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bornholm', 'region-sjaelland', 'danmark', 'storkoebenhavn', 'nordsjaelland', 'region-midtjylland', 'region-nordjylland', 'fyn', 'sydjylland'}\n",
      "[['Informationsteknologi', 'it'], ['Ingeniør og teknik', 'ingenioer'], ['Ledelse og personale', 'ledelse'], ['Handel og service', 'handel'], ['Industri og håndværk', 'industri'], ['Salg og kommunikation', 'salg'], ['Undervisning', 'undervisning'], ['Kontor og økonomi', 'kontor'], ['Social og sundhed', 'social'], ['Øvrige stillinger', 'oevrige']]\n",
      "['it/database', 'it/itdrift', 'it/itkurser', 'it/itledelse', 'it/internet', 'it/systemudvikling', 'it/telekom', 'it/virksomhedssystemer', 'ingenioer/byggeteknik', 'ingenioer/elektronik', 'ingenioer/kemi', 'ingenioer/teknikledelse', 'ingenioer/maskiningenioer', 'ingenioer/medicinal', 'ingenioer/produktionsteknik', 'ledelse/detailledelse', 'ledelse/freelancekonsulent', 'ledelse/hrkurser', 'it/itledelse', 'ledelse/institutions', 'ledelse/leder', 'ingenioer/teknikledelse', 'ledelse/personale', 'ledelse/projektledelse', 'ledelse/salgschef', 'ledelse/topledelse', 'ledelse/virksomhedsudvikling', 'ledelse/oekonomichef', 'handel/bud', 'handel/boernepasning', 'handel/detailhandel', 'ledelse/detailledelse', 'handel/ejendomsservice', 'handel/frisoer', 'handel/hotel', 'handel/rengoering', 'handel/service', 'handel/sikkerhed', 'industri/blik', 'industri/byggeri', 'industri/elektriker', 'industri/industri', 'industri/jern', 'industri/lager', 'industri/landbrug', 'industri/maling', 'industri/mekanik', 'industri/naeringsmiddel', 'industri/tekstil', 'industri/transport', 'industri/traeindustri', 'industri/toemrer', 'salg/design', 'salg/ejendomsmaegler', 'salg/grafisk', 'salg/kommunikation', 'salg/kultur', 'salg/marketing', 'salg/salg', 'salg/salgskurser', 'ledelse/salgschef', 'salg/franchise', 'salg/telemarketing', 'undervisning/bibliotek', 'undervisning/forskning', 'ledelse/institutions', 'undervisning/laerer', 'undervisning/paedagog', 'undervisning/voksenuddannelse', 'kontor/akademisk', 'salg/ejendomsmaegler', 'handel/ejendomsservice', 'kontor/finans', 'kontor/indkoeb', 'kontor/jura', 'kontor/kontor', 'kontor/kontorkurser', 'kontor/kontorelev', 'kontor/logistik', 'kontor/offentlig', 'kontor/oversaettelse', 'kontor/sekretaer', 'kontor/oekonomi', 'ledelse/oekonomichef', 'social/laege', 'social/laegesekretaer', 'kontor/offentlig', 'social/pleje', 'social/psykologi', 'social/socialraadgivning', 'social/sygeplejerske', 'social/tandlaege', 'social/teknisksundhed', 'social/terapi', 'oevrige/elev', 'oevrige/forsvar', 'oevrige/frivilligt', 'kontor/kontorelev', 'oevrige/student', 'oevrige/studiepraktik', 'oevrige/oevrige', 'oevrige/kurseroevrige']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import requests\n",
    "from datetime import date\n",
    "\n",
    "#Define the basic url\n",
    "url = 'https://www.jobindex.dk/jobsoegning'\n",
    "response = requests.get(url)\n",
    "html = response.text\n",
    "soup = BeautifulSoup(html,'lxml')\n",
    "\n",
    "#Areas are given in the html by div id=areas and class=area_label\n",
    "areas_html = soup.find('div', {'id':'areas'})#('a', {'class':'area_label'})\n",
    "areas_div = str(areas_html).split('href=\"/jobsoegning/')[1:]\n",
    "\n",
    "#Find the names of each of the areas\n",
    "areas = set()\n",
    "areas_delete = ('skaane', 'faeroeerne', 'udlandet', 'groenland') #delete regions outside Denmark's borders\n",
    "for area in areas_div:\n",
    "    link = area.split('\"')[0]\n",
    "    if link in areas_delete:\n",
    "        del link\n",
    "    else:\n",
    "        areas.add(link)\n",
    "print(areas)\n",
    "\n",
    "#Categories are given in the html by and id=categories\n",
    "cat_html = soup.find('div', {'id':'categories'})\n",
    "cat_div = soup.find_all('a',{'class':'filter-section-header title collapsed'}) #the filter collapses\n",
    "\n",
    "categories = []\n",
    "subcategories = []\n",
    "for cat in cat_div:\n",
    "    #first find the overall categories\n",
    "    cat_name = re.findall(r'(?<=<span>)(.*)(?=<span class)', str(cat))[0]\n",
    "    cat_name = cat_name.replace('\\xad','')\n",
    "    cat_links = re.findall(\"href=[\\\"\\'](.*?)[\\\"\\']\", str(cat))\n",
    "    cat_id = str(cat_links[0])\n",
    "    cat_id = cat_id.replace('#','')\n",
    "    category = [cat_name,cat_id]\n",
    "    categories.append(category)\n",
    "    #next find the sub-categories\n",
    "    subcat_div = str(cat_html).split('href=\"/jobsoegning/')[1:]\n",
    "for subcat in subcat_div:\n",
    "    subcategory = subcat.split('\"')[0]\n",
    "    #cat_id = re.findall('(.*?)/', str(subcategory))\n",
    "    #subcat_done = [subcategory,cat_id]\n",
    "    subcategories.append(subcategory)\n",
    "print(categories)\n",
    "print(subcategories)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping the areas and categories from jobindex.\n",
    "filter_jobs = []\n",
    "today = date.today().strftime(\"%Y%m%d\") # define today's date\n",
    "\n",
    "# loop through all areas and subcategories\n",
    "for area in areas:\n",
    "    for subcategory in subcategories:\n",
    "        print('Now scraping subcategory/area: ' + subcategory +' and '+ area)\n",
    "        url = f'https://www.jobindex.dk/jobsoegning/{subcategory}/{area}?jobage=archive&maxdate={today}&mindate=20080101'\n",
    "        response = requests.get(url)  \n",
    "        html = response.text  \n",
    "        soup = BeautifulSoup(html,'lxml')\n",
    "        \n",
    "        #Find the last page to put into loop\n",
    "        #pagination = soup.find_all('a', {'class':'page-link'})\n",
    "        #if len(pagination) == 0:\n",
    "        #    continue\n",
    "        #last_page = int(pagination[-1].text) \n",
    "'''    \n",
    "    # loop through pages\n",
    "        for page in range(1,last_page+1):\n",
    "            if  page % 25 == 0:\n",
    "                print('page: ',page,'/', last_page+1)\n",
    "            url = f'https://www.jobindex.dk/jobsoegning/{subcategory}/{area}?jobage=archive&maxdate={today}&mindate=20080101&page={page}'\n",
    "            response = requests.get(url)  \n",
    "            html = response.text   \n",
    "            if '<strong>' in html: \n",
    "                html = html.replace('<strong>','<b>')\\\n",
    "                       .replace('</strong>','</b>')\n",
    "            else:\n",
    "                print('No <strong>')\n",
    "            soup = BeautifulSoup(html,'lxml') \n",
    "'''    \n",
    "    #loop through job listings\n",
    "        joblistings = soup.find_all('div',{'class':'jobsearch-result'}) \n",
    "         #Find the total number of job listings to find max page number\n",
    "        no_jobs = joblistings.find('div',{'class':'jix_pagination_total'})\n",
    "        filter_jobs.append(no_jobs)\n",
    "        print(filter_jobs)\n",
    "'''       \n",
    "            for joblisting in joblistings:\n",
    "                title = re.findall(r'(?<=<b>)(.*)(?=</b>)', str(joblisting))[0]\n",
    "                if len(re.findall(r'(?<=<b>)(.*)(?=</b>)', str(joblisting)))>1:\n",
    "                    company = re.findall(r'(?<=<b>)(.*)(?=</b>)', str(joblisting))[1]\n",
    "                    if 'amp;' in company:\n",
    "                        company = company.replace('amp;','') \n",
    "                else:\n",
    "                    company = ''\n",
    "                pub_date = re.findall(r'time\\sdatetime=\"(.*)\"', str(joblisting))\n",
    "                geo_area = area\n",
    "                cat_id = re.findall('(.*?)/', str(subcategory))\n",
    "                subcat_id = subcategory\n",
    "                job = [title, company, pub_date, geo_area, cat_id, subcat_id]\n",
    "                filter_jobs.append(job)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Publication Date</th>\n",
       "      <th>Geographical Area</th>\n",
       "      <th>Job Category</th>\n",
       "      <th>Job Subcategory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Ørsted</td>\n",
       "      <td>[2020-08-21]</td>\n",
       "      <td>danmark</td>\n",
       "      <td>[it]</td>\n",
       "      <td>it/database</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Business Intelligence-konsulent til specialise...</td>\n",
       "      <td>Jobindex A/S</td>\n",
       "      <td>[2020-08-19]</td>\n",
       "      <td>danmark</td>\n",
       "      <td>[it]</td>\n",
       "      <td>it/database</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data warehouse-arkitekt med erfaring inden for...</td>\n",
       "      <td>Nykredit</td>\n",
       "      <td>[2020-08-19]</td>\n",
       "      <td>danmark</td>\n",
       "      <td>[it]</td>\n",
       "      <td>it/database</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Business Intelligence Arkitekt</td>\n",
       "      <td>Systematic A/S</td>\n",
       "      <td>[2020-08-17]</td>\n",
       "      <td>danmark</td>\n",
       "      <td>[it]</td>\n",
       "      <td>it/database</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Business Intelligence Backend Developer</td>\n",
       "      <td>Systematic A/S</td>\n",
       "      <td>[2020-08-17]</td>\n",
       "      <td>danmark</td>\n",
       "      <td>[it]</td>\n",
       "      <td>it/database</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>Data Center Operations Specialist at NNIT</td>\n",
       "      <td>NNIT&lt;/b&gt;, &lt;b&gt;Søborg</td>\n",
       "      <td>[2020-07-24]</td>\n",
       "      <td>danmark</td>\n",
       "      <td>[it]</td>\n",
       "      <td>it/itdrift</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>It-konsulent</td>\n",
       "      <td>CESCOM IT A/S&lt;/b&gt;, &lt;b&gt;Aars</td>\n",
       "      <td>[2020-07-24]</td>\n",
       "      <td>danmark</td>\n",
       "      <td>[it]</td>\n",
       "      <td>it/itdrift</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>Shelter and Infrastructure Assistant (Technical)</td>\n",
       "      <td>Dansk Flygtningehjælp&lt;/b&gt;, &lt;b&gt;Danmark</td>\n",
       "      <td>[2020-07-24]</td>\n",
       "      <td>danmark</td>\n",
       "      <td>[it]</td>\n",
       "      <td>it/itdrift</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>Infrastructure and Operations Specialist</td>\n",
       "      <td>Enghouse&lt;/b&gt;, &lt;b&gt;Aarhus</td>\n",
       "      <td>[2020-07-24]</td>\n",
       "      <td>danmark</td>\n",
       "      <td>[it]</td>\n",
       "      <td>it/itdrift</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>DPA-System søger en IT- og systemadministrator</td>\n",
       "      <td>VIGA Recruitment&lt;/b&gt;, &lt;b&gt;København</td>\n",
       "      <td>[2020-07-24]</td>\n",
       "      <td>danmark</td>\n",
       "      <td>[it]</td>\n",
       "      <td>it/itdrift</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>280 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Job Title  \\\n",
       "0                                Senior Data Scientist   \n",
       "1    Business Intelligence-konsulent til specialise...   \n",
       "2    Data warehouse-arkitekt med erfaring inden for...   \n",
       "3                       Business Intelligence Arkitekt   \n",
       "4              Business Intelligence Backend Developer   \n",
       "..                                                 ...   \n",
       "275          Data Center Operations Specialist at NNIT   \n",
       "276                                       It-konsulent   \n",
       "277   Shelter and Infrastructure Assistant (Technical)   \n",
       "278           Infrastructure and Operations Specialist   \n",
       "279     DPA-System søger en IT- og systemadministrator   \n",
       "\n",
       "                                   Company Publication Date Geographical Area  \\\n",
       "0                                   Ørsted     [2020-08-21]           danmark   \n",
       "1                             Jobindex A/S     [2020-08-19]           danmark   \n",
       "2                                 Nykredit     [2020-08-19]           danmark   \n",
       "3                           Systematic A/S     [2020-08-17]           danmark   \n",
       "4                           Systematic A/S     [2020-08-17]           danmark   \n",
       "..                                     ...              ...               ...   \n",
       "275                    NNIT</b>, <b>Søborg     [2020-07-24]           danmark   \n",
       "276             CESCOM IT A/S</b>, <b>Aars     [2020-07-24]           danmark   \n",
       "277  Dansk Flygtningehjælp</b>, <b>Danmark     [2020-07-24]           danmark   \n",
       "278                Enghouse</b>, <b>Aarhus     [2020-07-24]           danmark   \n",
       "279     VIGA Recruitment</b>, <b>København     [2020-07-24]           danmark   \n",
       "\n",
       "    Job Category Job Subcategory  \n",
       "0           [it]     it/database  \n",
       "1           [it]     it/database  \n",
       "2           [it]     it/database  \n",
       "3           [it]     it/database  \n",
       "4           [it]     it/database  \n",
       "..           ...             ...  \n",
       "275         [it]      it/itdrift  \n",
       "276         [it]      it/itdrift  \n",
       "277         [it]      it/itdrift  \n",
       "278         [it]      it/itdrift  \n",
       "279         [it]      it/itdrift  \n",
       "\n",
       "[280 rows x 6 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Make into pandas dataframe\n",
    "import pandas as pd\n",
    "filter_data = pd.DataFrame(filter_jobs)\n",
    "filter_data.columns = [\"Job Title\", \"Company\", \"Publication Date\", \"Geographical Area\", \"Job Category\", \"Job Subcategory\"]\n",
    "filter_data\n",
    "\n",
    "#find duplicates within area\n",
    "area_duplicates = filter_data[filter_data.duplicated(['Name'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
