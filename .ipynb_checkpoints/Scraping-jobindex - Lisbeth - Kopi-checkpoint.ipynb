{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exam project: Scraping job postings from Jobindex\n",
    "\n",
    "This notebook is used for webscraping www.jobindex.dk for all job postings in the period xxxx-2020. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "#Make log\n",
    "import scraping_class\n",
    "logfile = 'log_jobindex_scraping.txt'\n",
    "connector = scraping_class.Connector(logfile)\n",
    "\n",
    "#Define todays date\n",
    "from datetime import date\n",
    "today = date.today().strftime(\"%Y%m%d\") # to get format 20080101"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the maximum page number to do a FOR loop through\n",
    "Each page of www.jobindex.dk/jobsoegning contains 20 joblistings. Since the number of job postings differ from time to time, so does the number of pages. To be able to scrape job postings on all pages, we look at the pagination. The pagination looks as the picture below, and we want to save the number of the last page as \"last_page\" to be able to loop through it later on.  \n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "quarters = [['0101','0331'],['0401','0630'],['0701','0930'],['1001','1231']]\n",
    "\n",
    "#Define the 'quarter 'url and find the max number of pages for that quarter\n",
    "for y in range(10,11):\n",
    "    for q in quarters:\n",
    "        url = f'https://www.jobindex.dk/jobsoegning?maxdate=20{y}{q[1]}&mindate=20{y}{q[0]}&jobage=archive'\n",
    "        response = requests.get(url)   \n",
    "        soup = BeautifulSoup(response.text,'lxml') # parse the raw html using BeautifoulSoup\n",
    "        pages = BeautifulSoup(str(soup.find_all(\"div\", {'class':'jix_pagination_total'}))).get_text()\n",
    "        total = re.findall(r'af(?s)(.*)resultater',str(pages))\n",
    "        total = str(total).strip(\"[]\").strip(\"''\").strip().replace('.','')\n",
    "        last_page = math.ceil(int(total)/20)+1\n",
    "        jobindex_links = []\n",
    "        for i in range(1,last_page):\n",
    "            url = f'https://www.jobindex.dk/jobsoegning?maxdate=20{y}{q[1]}&mindate=20{y}{q[0]}&page={i}&jobage=archive'\n",
    "            jobindex_links.append(url)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract all job postings from www.jobindex.dk. \n",
    "For each job posting one is redirected from www.jobindex.dk/jobsoegning to a separate www-webpage or PDF, that contains the full job posting. We therefore have to distinguish between these two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import PyPDF2 \n",
    "import textract\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import slate3k as slate\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping url 0 out of 2161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lisbe\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1004: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.jobindex.dk'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n",
      "WARNING:root:Parser index out of bounds\n",
      "WARNING:root:Parser index out of bounds\n",
      "WARNING:root:Literal required: CIDSystemInfo\n",
      "WARNING:root:Literal required: CIDSystemInfo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping url 1 out of 2161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lisbe\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1004: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.jobindex.dk'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "HTTPConnectionPool(host='92.43.126.165', port=80): Max retries exceeded with url: /Candidate-page/JobAdList/JobAd/?jaid=29 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001E70B0C0C08>: Failed to establish a new connection: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond'))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    156\u001b[0m             conn = connection.create_connection(\n\u001b[1;32m--> 157\u001b[1;33m                 \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dns_host\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mextra_kw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m             )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\util\\connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0merr\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\util\\connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     73\u001b[0m                 \u001b[0msock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             \u001b[0msock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msa\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msock\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTimeoutError\u001b[0m: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    671\u001b[0m                 \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 672\u001b[1;33m                 \u001b[0mchunked\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchunked\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    673\u001b[0m             )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    386\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 387\u001b[1;33m             \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mhttplib_request_kw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    388\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1251\u001b[0m         \u001b[1;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1252\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_send_request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1297\u001b[0m             \u001b[0mbody\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'body'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1298\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mendheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1246\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1247\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1025\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1026\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1027\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    965\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 966\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    967\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    183\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 184\u001b[1;33m         \u001b[0mconn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    185\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prepare_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    168\u001b[0m             raise NewConnectionError(\n\u001b[1;32m--> 169\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Failed to establish a new connection: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m             )\n",
      "\u001b[1;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x000001E70B0C0C08>: Failed to establish a new connection: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m                     \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m                 )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    719\u001b[0m             retries = retries.increment(\n\u001b[1;32m--> 720\u001b[1;33m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    721\u001b[0m             )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\util\\retry.py\u001b[0m in \u001b[0;36mincrement\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    435\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnew_retry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_exhausted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 436\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    437\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='92.43.126.165', port=80): Max retries exceeded with url: /Candidate-page/JobAdList/JobAd/?jaid=29 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001E70B0C0C08>: Failed to establish a new connection: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-166-ff02460cdfb4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     89\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m                     \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlink\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m                     \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m                     \u001b[0mhtml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m                     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lxml'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'allow_redirects'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'get'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    531\u001b[0m         }\n\u001b[0;32m    532\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 533\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    534\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    644\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 646\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    647\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    514\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 516\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    517\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mClosedPoolError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPConnectionPool(host='92.43.126.165', port=80): Max retries exceeded with url: /Candidate-page/JobAdList/JobAd/?jaid=29 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001E70B0C0C08>: Failed to establish a new connection: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond'))"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "quarters = [['0101','0331'],['0401','0630'],['0701','0930'],['1001'],['1231']]\n",
    "\n",
    "#Define the 'quarter 'url and find the max number of pages for that quarter\n",
    "\n",
    "# Loop through each year\n",
    "jobs = [] \n",
    "for y in range(10,11): # testing for 2010 \n",
    "    # Loop through each quarter\n",
    "    for q in quarters:\n",
    "        # define quarterly search link\n",
    "        url = f'https://www.jobindex.dk/jobsoegning?maxdate=20{y}{q[1]}&mindate=20{y}{q[0]}&jobage=archive'\n",
    "        response = requests.get(url)   \n",
    "        soup = BeautifulSoup(response.text,'lxml') # parse the raw html using BeautifoulSoup\n",
    "        # identify total number of job postings in the given quarter\n",
    "        pages = BeautifulSoup(str(soup.find_all(\"div\", {'class':'jix_pagination_total'}))).get_text() \n",
    "        total = re.findall(r'af(?s)(.*)resultater',str(pages))\n",
    "        total = str(total).strip(\"[]\").strip(\"''\").strip().replace('.','')\n",
    "        # use total number of job postings to calculate total number of pages\n",
    "        last_page = math.ceil(int(total)/20)+1\n",
    "        jobindex_links = []\n",
    "        for i in range(1,last_page):\n",
    "            url = f'https://www.jobindex.dk/jobsoegning?maxdate=20{y}{q[1]}&mindate=20{y}{q[0]}&page={i}&jobage=archive'\n",
    "            jobindex_links.append(url)       \n",
    "        html = []\n",
    "        job_links = []\n",
    "        for url in jobindex_links:\n",
    "            url_index = jobindex_links.index(url)\n",
    "            print('scraping url',url_index,'out of',len(jobindex_links))\n",
    "            response = ''\n",
    "            while response == '':\n",
    "                try:\n",
    "                    response = requests.get(url, verify = False)\n",
    "                    break\n",
    "                except:\n",
    "                    print(\"Connection refused by the server..\")\n",
    "                    print(\"Let me sleep for 5 seconds\")\n",
    "                    print(\"ZZzzzz...\")\n",
    "                    time.sleep(5)\n",
    "                    print(\"Was a nice sleep, now let me continue...\")\n",
    "                    continue\n",
    "            html = response.text    \n",
    "            if '<strong>' in html: \n",
    "                html = html.replace('<strong>','<b>')\\\n",
    "                        .replace('</strong>','</b>')\n",
    "            soup = BeautifulSoup(html,'lxml') \n",
    "            #one job result is given by class=jobsearch-result\n",
    "            joblistings = soup.find_all('div',{'class':'jobsearch-result'})\n",
    "            for joblisting in joblistings:\n",
    "                title = re.findall(r'(?<=<b>)(.*)(?=</b>)', str(joblisting))[0]\n",
    "                if len(re.findall(r'(?<=<b>)(.*)(?=</b>)', str(joblisting)))>1:\n",
    "                    company = re.findall(r'(?<=<b>)(.*)(?=</b>)', str(joblisting))[1]\n",
    "                    if 'amp;' in company:\n",
    "                                company = company.replace('amp;','') \n",
    "                else:\n",
    "                    company = ''\n",
    "                pub_date = re.findall(r'time\\sdatetime=\"(.*)\"', str(joblisting))\n",
    "                descrip = BeautifulSoup(str(joblisting), 'lxml')\n",
    "                for div in descrip.find_all(\"div\", {'class':'jix_toolbar jix_appetizer_toolbar'}): # remove toolbar at the end of each job\n",
    "                    div.decompose()\n",
    "                for span in descrip.find_all(\"span\", {'class':'jix_toolbar jix_appetizer_toolbar'}): # remove toolbar at the end of each job\n",
    "                    span.decompose()\n",
    "                links = re.findall(\"href=[\\\"\\'](.*?)[\\\"\\']\", str(descrip))\n",
    "                if len(links)>1:\n",
    "                    link = str(links[1])\n",
    "                else:\n",
    "                    link = str(links[0])\n",
    "                link = link.replace(\"&amp;\", \"&\")\n",
    "                # Indirectly extracting the short job description by removing everything else\n",
    "                descrip = descrip.get_text()\n",
    "                descrip = descrip.replace(\"\\n\", \" \") # remove \\n \n",
    "                job = [title,link,pub_date,descrip,company]\n",
    "                 # Jobs is a list of list where each list contains the title job, and the link for that job, this will help later\n",
    "                if 'pdf' in link:\n",
    "                    #scraping a PDF file\n",
    "                    #request the url\n",
    "                    url = link\n",
    "                    response = requests.get(url)\n",
    "                    #save the pdf in the current folder\n",
    "                    with open('pdf.pdf', 'wb') as f:\n",
    "                        f.write(response.content)\n",
    "                    #open the pdf and get the text in doc\n",
    "                    with open('pdf.pdf', 'rb') as fp:\n",
    "                        doc = slate.PDF(fp)\n",
    "                        doc = str(doc).replace('\\n','')\n",
    "                        doc = str(doc).replace('\\r','')\n",
    "                        job.append(doc)\n",
    "                else:\n",
    "                    url = link\n",
    "                    response = requests.get(url)\n",
    "                    html = response.text\n",
    "                    soup = BeautifulSoup(html, 'lxml')\n",
    "                    description = soup.get_text()\n",
    "                    description = description.replace('\\n','')\n",
    "                    description = description.replace('\\r','')\n",
    "                    job.append(description)\n",
    "                jobs.append(job)\n",
    "        print('Finished',q,'of year 20',y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforming into dataframe\n",
    "df = pd.DataFrame(jobs)\n",
    "df.columns = [\"Job Title\",\"Link\",\"Date\", \"Short_Description\",\"Company\", \"Full_Description\"]\n",
    "\n",
    "df.to_pickle(r'C:\\Users\\lisbe\\OneDrive\\11. Semester\\Social Data Science\\Modules\\Jobs_basic_info')\n",
    "unpickled_df = pd.read_pickle(r\"C:\\Users\\lisbe\\OneDrive\\11. Semester\\Social Data Science\\Modules\\Jobs_basic_info\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying jobs with possibility of working remotely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we define a list of keyword that we think a job posting where working remotely is a possibility will include\n",
    "\n",
    "keywords = ['hjemmefra', 'arbejde hjemme', 'hjemmearbejde','hjemmekontor', 'arbejde hjemmefra', 'arbejde remote', 'fjernarbejde' //\n",
    "           'remote-arbejdsplads', 'remote work', 'work from home', 'work remotely']\n",
    "\n",
    "# I will now look for these words in the description column. I am not using the tokenized version of column because then I \n",
    "# can't look for expressions with more than one word. If you want to use the tokenized version then you have to use bigrams\n",
    "\n",
    "df['Remote'] = '0'\n",
    "for word in keywords:\n",
    "    for i in range(len(df)):\n",
    "        df['Description'][i] = str(df['Description'][i]).lower()\n",
    "        if word in df['Description'][i]:\n",
    "            df['Remote'][i] = '1'\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying the most popular words in job descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first step is to clean the job descriptions as much as possible so we can narrow down the words included for \n",
    "# analysis as much as possible\n",
    "\n",
    "df['Tokenized_description'] = ' '\n",
    "symbols = ['?','!','>','<','-','[',']','(',')','{','}',' –','``',\"''\",'\"\"','\\\\','@','$','&','=']\n",
    "for i in range(len(df)):\n",
    "    stop_words = nltk.corpus.stopwords.words(\"danish\")\n",
    "    df['Tokenized_description'][i] = re.sub(r'(\\.+ )|,|\\||:|/|\\'|\\-|;|\\*|!|(\\s\\d+\\s)|(\\s\\W\\s)',' ',str(df['Description'][i]))\n",
    "    df['Tokenized_description'][i] = str(df['Tokenized_description'][i]).rstrip('\\\\')\n",
    "    df['Tokenized_description'][i] = nltk.word_tokenize(str(df['Tokenized_description'][i].lower()))\n",
    "    df['Tokenized_description'][i] = [w for w in df['Tokenized_description'][i] if not w in stop_words] \n",
    "    df['Tokenized_description'][i] = [w for w in df['Tokenized_description'][i] if not w in symbols]\n",
    "    df['Tokenized_description'][i] = [w for w in df['Tokenized_description'][i] if not w.isnumeric()]\n",
    "    df['Tokenized_description'][i] = [word for word in df['Tokenized_description'][i] if len(word) > 3]\n",
    "    \n",
    "# Finding intersections --- USELESS\n",
    "df['description_set'] = df['Tokenized_description'].apply(set)\n",
    "description_set = df['description_set'].tolist() #convert each description into a set \n",
    "set.intersection(*description_set)\n",
    "del df['description_set']\n",
    "\n",
    "# Finding most frequent words in all descriptions --- USELESS ATM BUT MAYBE IF WE CLEAN IT ENOUGH IT WILL WORK\n",
    "\n",
    "descriptions_list = [] \n",
    "# This loop will pull all tokens in one bag\n",
    "for i in range(len(df)):\n",
    "    descriptions_list.extend(df['Tokenized_description'][i]) \n",
    "    \n",
    "word_dist = nltk.FreqDist(descriptions_list) word_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL DEDICATED TO KEEP CLEANING DESCRIPTIONS\n",
    "\n",
    "def clean(doc):\n",
    "    #doc = doc.replace('\\n','')\n",
    "    #doc = doc.replace('\\r','')\n",
    "    doc = doc.replace('\\t','')\n",
    "    doc = doc.replace('\\'','')\n",
    "    doc = doc.replace('|','')\n",
    "    doc = doc.replace('/','')\n",
    "\n",
    "def strip_html(row):\n",
    "    return str(html.fromstring(row).text_content())\n",
    "\n",
    "from lxml import html\n",
    "for row in df['Description']:\n",
    "    strip_html(str(row))\n",
    "    \n",
    "    \n",
    "    \n",
    "#text = re.sub('<[^>]*>', '', text)\n",
    "#emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assigning DISCO-08 codes to the job postings\n",
    "This function will be used to assign a DISCO-08 code to the job postings. It goes through the following steps:\n",
    "\n",
    "1. Importing a csv file downloaded from Danmarks Statistik which includes the different job functions that fall under each DISCO-08 category\n",
    "2. Clean this csv file and converting it into a dataframe with two columns, one that has the disco codes and one that has a list of all the jobs that fall into each code\n",
    "3. Tokenizing the jobs \n",
    "4. Tokenizing the words in the 'Job Title' column from our job postings data\n",
    "5. Creating a function that compares the tokens in the job titles to the tokens in the DISCO dataframe and if a match is found, then returning the corresponding DISCO code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lisbe\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "# Importing and cleaning DISCO-08 classification\n",
    "import pandas as pd\n",
    "import re\n",
    "disco = pd.read_csv(r\"C:\\Users\\lisbe\\OneDrive\\11. Semester\\Social Data Science\\Modules\\csv_da.csv\",header=None)\n",
    "disco_clean = disco.copy()\n",
    "\n",
    "# Remove words that start with lowercase\n",
    "for i in range(len(disco_clean)):\n",
    "    disco_clean[3][i] = ' '.join([word for word in str(disco_clean[3][i]).split(' ') if not word.islower()])\n",
    "\n",
    "# Create a dictionary for DISCO functions\n",
    "\n",
    "disco_clean['DISCO'] = disco_clean[0].astype(str).str[:1] #This column will have the highest hierarchy code 0-9\n",
    "del disco_clean[0]\n",
    "del disco_clean[1]\n",
    "del disco_clean[2]\n",
    "disco_clean = disco_clean.groupby(by=disco_clean['DISCO']).sum()\n",
    "disco_clean.rename({3:'functions'}, axis='columns', inplace = True) # rename column \n",
    "for i in range(9): # Adding a space in between words that are missing them \"LikeThis\"\n",
    "    disco_clean['DISCO'] = disco_clean['DISCO'].astype(str).str[:1] # We only want the first number \n",
    "    disco_clean['functions'][i] = re.sub(r\"(\\w)([A-Z])\", r\"\\1 \\2\", str(disco_clean['functions'][i]))\n",
    "    \n",
    "disco_clean[\"functions_tokenized\"] = \"\"\n",
    "for i in range(len(disco_clean)):\n",
    "    disco_clean['functions_tokenized'][i]=nltk.word_tokenize(str(disco_clean['functions'][i]).lower())\n",
    "    \n",
    "# Now I would like a job type column from the job titles where I would extract the nouns from job titles\n",
    "#pip install afinn\n",
    "import nltk\n",
    "df['Job Title']=df['Job Title'].str.replace(',','')\n",
    "df[\"tokenized_titles\"] = \"\"\n",
    "for i in range(len(df)):\n",
    "    df['tokenized_titles'][i] = nltk.word_tokenize(str(df['Job Title'][i]).lower())\n",
    "\n",
    "# This loop creates a new column in the df that assigns a disco code to each posting based on the words in the title\n",
    "\n",
    "df['disco'] = ''\n",
    "for m in range(len(df)):\n",
    "    for i in range(len(disco_clean)):\n",
    "        for element in disco_clean['functions_tokenized'][i]:\n",
    "            if element in df['tokenized_titles'][m]:\n",
    "                df['disco'][m] = disco_clean['DISCO'][i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape the joblistings for each geographical area and job categories/industries\n",
    "www.jobindex.dk/jobsoegning has a filter for geographical area and job category, respectively. These can be seen in the picture below. We will scrape this to be able to describe in which areas and industries remote working is especially taking place. The address of the company is also attached to the job posting, and even though this information would be more detailed, we have experienced that the address often belongs to the headquarter of the company and not the specific area, where the job is. Therefore we use the information from the geographical filter.\n",
    "We will loop through the filters and extract job titles and company, which can then be merged onto the big dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'danmark', 'region-midtjylland', 'nordsjaelland', 'sydjylland', 'bornholm', 'storkoebenhavn', 'region-nordjylland', 'fyn', 'region-sjaelland'}\n",
      "[['Informationsteknologi', 'it'], ['Ingeniør og teknik', 'ingenioer'], ['Ledelse og personale', 'ledelse'], ['Handel og service', 'handel'], ['Industri og håndværk', 'industri'], ['Salg og kommunikation', 'salg'], ['Undervisning', 'undervisning'], ['Kontor og økonomi', 'kontor'], ['Social og sundhed', 'social'], ['Øvrige stillinger', 'oevrige']]\n",
      "['it/database', 'it/itdrift', 'it/itkurser', 'it/itledelse', 'it/internet', 'it/systemudvikling', 'it/telekom', 'it/virksomhedssystemer', 'ingenioer/byggeteknik', 'ingenioer/elektronik', 'ingenioer/kemi', 'ingenioer/teknikledelse', 'ingenioer/maskiningenioer', 'ingenioer/medicinal', 'ingenioer/produktionsteknik', 'ledelse/detailledelse', 'ledelse/freelancekonsulent', 'ledelse/hrkurser', 'it/itledelse', 'ledelse/institutions', 'ledelse/leder', 'ingenioer/teknikledelse', 'ledelse/personale', 'ledelse/projektledelse', 'ledelse/salgschef', 'ledelse/topledelse', 'ledelse/virksomhedsudvikling', 'ledelse/oekonomichef', 'handel/bud', 'handel/boernepasning', 'handel/detailhandel', 'ledelse/detailledelse', 'handel/ejendomsservice', 'handel/frisoer', 'handel/hotel', 'handel/rengoering', 'handel/service', 'handel/sikkerhed', 'industri/blik', 'industri/byggeri', 'industri/elektriker', 'industri/industri', 'industri/jern', 'industri/lager', 'industri/landbrug', 'industri/maling', 'industri/mekanik', 'industri/naeringsmiddel', 'industri/tekstil', 'industri/transport', 'industri/traeindustri', 'industri/toemrer', 'salg/design', 'salg/ejendomsmaegler', 'salg/grafisk', 'salg/kommunikation', 'salg/kultur', 'salg/marketing', 'salg/salg', 'salg/salgskurser', 'ledelse/salgschef', 'salg/franchise', 'salg/telemarketing', 'undervisning/bibliotek', 'undervisning/forskning', 'ledelse/institutions', 'undervisning/laerer', 'undervisning/paedagog', 'undervisning/voksenuddannelse', 'kontor/akademisk', 'salg/ejendomsmaegler', 'handel/ejendomsservice', 'kontor/finans', 'kontor/indkoeb', 'kontor/jura', 'kontor/kontor', 'kontor/kontorkurser', 'kontor/kontorelev', 'kontor/logistik', 'kontor/offentlig', 'kontor/oversaettelse', 'kontor/sekretaer', 'kontor/oekonomi', 'ledelse/oekonomichef', 'social/laege', 'social/laegesekretaer', 'kontor/offentlig', 'social/pleje', 'social/psykologi', 'social/socialraadgivning', 'social/sygeplejerske', 'social/tandlaege', 'social/teknisksundhed', 'social/terapi', 'oevrige/elev', 'oevrige/forsvar', 'oevrige/frivilligt', 'kontor/kontorelev', 'oevrige/student', 'oevrige/studiepraktik', 'oevrige/oevrige', 'oevrige/kurseroevrige']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import requests\n",
    "from datetime import date\n",
    "\n",
    "#Define the basic url\n",
    "url = 'https://www.jobindex.dk/jobsoegning'\n",
    "response = requests.get(url)\n",
    "html = response.text\n",
    "soup = BeautifulSoup(html,'lxml')\n",
    "\n",
    "#Areas are given in the html by div id=areas and class=area_label\n",
    "areas_html = soup.find('div', {'id':'areas'})#('a', {'class':'area_label'})\n",
    "areas_div = str(areas_html).split('href=\"/jobsoegning/')[1:]\n",
    "\n",
    "#Find the names of each of the areas\n",
    "areas = set()\n",
    "areas_delete = ('skaane', 'faeroeerne', 'udlandet', 'groenland') #delete regions outside Denmark's borders\n",
    "for area in areas_div:\n",
    "    link = area.split('\"')[0]\n",
    "    if link in areas_delete:\n",
    "        del link\n",
    "    else:\n",
    "        areas.add(link)\n",
    "print(areas)\n",
    "\n",
    "#Categories are given in the html by and id=categories\n",
    "cat_html = soup.find('div', {'id':'categories'})\n",
    "cat_div = soup.find_all('a',{'class':'filter-section-header title collapsed'}) #the filter collapses\n",
    "\n",
    "categories = []\n",
    "subcategories = []\n",
    "for cat in cat_div:\n",
    "    #first find the overall categories\n",
    "    cat_name = re.findall(r'(?<=<span>)(.*)(?=<span class)', str(cat))[0]\n",
    "    cat_name = cat_name.replace('\\xad','')\n",
    "    cat_links = re.findall(\"href=[\\\"\\'](.*?)[\\\"\\']\", str(cat))\n",
    "    cat_id = str(cat_links[0])\n",
    "    cat_id = cat_id.replace('#','')\n",
    "    category = [cat_name,cat_id]\n",
    "    categories.append(category)\n",
    "    #next find the sub-categories\n",
    "    subcat_div = str(cat_html).split('href=\"/jobsoegning/')[1:]\n",
    "for subcat in subcat_div:\n",
    "    subcategory = subcat.split('\"')[0]\n",
    "    #cat_id = re.findall('(.*?)/', str(subcategory))\n",
    "    #subcat_done = [subcategory,cat_id]\n",
    "    subcategories.append(subcategory)\n",
    "print(categories)\n",
    "print(sub_categories)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now scraping subcategory/area: it/database and danmark\n",
      "Now scraping subcategory/area: it/itdrift and danmark\n",
      "Now scraping subcategory/area: it/itkurser and danmark\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-8cf4e36278aa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'page: '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_page\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf'https://www.jobindex.dk/jobsoegning/{subcategory}/{area}?jobage=archive&maxdate={today}&mindate=20080101&page={page}'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m             \u001b[0mhtml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;34m'<strong>'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhtml\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'allow_redirects'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'get'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    528\u001b[0m         }\n\u001b[0;32m    529\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    531\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    641\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    642\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 643\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    644\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m                 resp = conn.urlopen(\n\u001b[0m\u001b[0;32m    440\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m                     \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    668\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m             \u001b[1;31m# Make the request on the httplib connection object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 670\u001b[1;33m             httplib_response = self._make_request(\n\u001b[0m\u001b[0;32m    671\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    424\u001b[0m                     \u001b[1;31m# Python 3 (including for exceptions like SystemExit).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m                     \u001b[1;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 426\u001b[1;33m                     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    427\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    419\u001b[0m                 \u001b[1;31m# Python 3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 421\u001b[1;33m                     \u001b[0mhttplib_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    422\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m                     \u001b[1;31m# Remove the TypeError from the exception chain in\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1330\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1331\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1332\u001b[1;33m                 \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1333\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1334\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mbegin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    301\u001b[0m         \u001b[1;31m# read until we get a non-100 response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 303\u001b[1;33m             \u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    304\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 264\u001b[1;33m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"iso-8859-1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    265\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"status line\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    667\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 669\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    670\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1239\u001b[0m                   \u001b[1;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1240\u001b[0m                   self.__class__)\n\u001b[1;32m-> 1241\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1242\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1243\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1097\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1099\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1100\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1101\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Scraping the areas and categories from jobindex.\n",
    "filter_jobs = []\n",
    "today = date.today().strftime(\"%Y%m%d\") # define today's date\n",
    "\n",
    "# loop through all areas and subcategories\n",
    "for area in areas:\n",
    "    for subcategory in subcategories:\n",
    "        print('Now scraping subcategory/area: ' + subcategory +' and '+ area)\n",
    "        url = f'https://www.jobindex.dk/jobsoegning/{subcategory}/{area}?jobage=archive&maxdate={today}&mindate=20080101'\n",
    "        response = requests.get(url)  \n",
    "        html = response.text  \n",
    "        soup = BeautifulSoup(html,'lxml')\n",
    "        \n",
    "        #Find the last page to put into loop\n",
    "        pagination = soup.find_all('a', {'class':'page-link'})\n",
    "        if len(pagination) == 0:\n",
    "            continue\n",
    "        last_page = int(pagination[-1].text) \n",
    "\n",
    "    # loop through pages\n",
    "        for page in range(1,last_page+1):\n",
    "            if  page % 25 == 0:\n",
    "                print('page: ',page,'/', last_page+1)\n",
    "            url = f'https://www.jobindex.dk/jobsoegning/{subcategory}/{area}?jobage=archive&maxdate={today}&mindate=20080101&page={page}'\n",
    "            response = requests.get(url)  \n",
    "            html = response.text   \n",
    "            if '<strong>' in html: \n",
    "                html = html.replace('<strong>','<b>')\\\n",
    "                       .replace('</strong>','</b>')\n",
    "            else:\n",
    "                print('No <strong>')\n",
    "            soup = BeautifulSoup(html,'lxml') \n",
    "    \n",
    "    #loop through job listings\n",
    "            joblistings = soup.find_all('div',{'class':'jobsearch-result'}) \n",
    "            for joblisting in joblistings:\n",
    "                title = re.findall(r'(?<=<b>)(.*)(?=</b>)', str(joblisting))[0]\n",
    "                if len(re.findall(r'(?<=<b>)(.*)(?=</b>)', str(joblisting)))>1:\n",
    "                    company = re.findall(r'(?<=<b>)(.*)(?=</b>)', str(joblisting))[1]\n",
    "                    if 'amp;' in company:\n",
    "                        company = company.replace('amp;','') \n",
    "                else:\n",
    "                    company = ''\n",
    "                pub_date = re.findall(r'time\\sdatetime=\"(.*)\"', str(joblisting))\n",
    "                geo_area = area\n",
    "                cat_id = re.findall('(.*?)/', str(subcategory))\n",
    "                subcat_id = subcategory\n",
    "                job = [title, company, pub_date, geo_area, cat_id, subcat_id]\n",
    "                filter_jobs.append(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Publication Date</th>\n",
       "      <th>Geographical Area</th>\n",
       "      <th>Job Category</th>\n",
       "      <th>Job Subcategory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Ørsted</td>\n",
       "      <td>[2020-08-21]</td>\n",
       "      <td>danmark</td>\n",
       "      <td>[it]</td>\n",
       "      <td>it/database</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Business Intelligence-konsulent til specialise...</td>\n",
       "      <td>Jobindex A/S</td>\n",
       "      <td>[2020-08-19]</td>\n",
       "      <td>danmark</td>\n",
       "      <td>[it]</td>\n",
       "      <td>it/database</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data warehouse-arkitekt med erfaring inden for...</td>\n",
       "      <td>Nykredit</td>\n",
       "      <td>[2020-08-19]</td>\n",
       "      <td>danmark</td>\n",
       "      <td>[it]</td>\n",
       "      <td>it/database</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Business Intelligence Arkitekt</td>\n",
       "      <td>Systematic A/S</td>\n",
       "      <td>[2020-08-17]</td>\n",
       "      <td>danmark</td>\n",
       "      <td>[it]</td>\n",
       "      <td>it/database</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Business Intelligence Backend Developer</td>\n",
       "      <td>Systematic A/S</td>\n",
       "      <td>[2020-08-17]</td>\n",
       "      <td>danmark</td>\n",
       "      <td>[it]</td>\n",
       "      <td>it/database</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>Data Center Operations Specialist at NNIT</td>\n",
       "      <td>NNIT&lt;/b&gt;, &lt;b&gt;Søborg</td>\n",
       "      <td>[2020-07-24]</td>\n",
       "      <td>danmark</td>\n",
       "      <td>[it]</td>\n",
       "      <td>it/itdrift</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>It-konsulent</td>\n",
       "      <td>CESCOM IT A/S&lt;/b&gt;, &lt;b&gt;Aars</td>\n",
       "      <td>[2020-07-24]</td>\n",
       "      <td>danmark</td>\n",
       "      <td>[it]</td>\n",
       "      <td>it/itdrift</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>Shelter and Infrastructure Assistant (Technical)</td>\n",
       "      <td>Dansk Flygtningehjælp&lt;/b&gt;, &lt;b&gt;Danmark</td>\n",
       "      <td>[2020-07-24]</td>\n",
       "      <td>danmark</td>\n",
       "      <td>[it]</td>\n",
       "      <td>it/itdrift</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>Infrastructure and Operations Specialist</td>\n",
       "      <td>Enghouse&lt;/b&gt;, &lt;b&gt;Aarhus</td>\n",
       "      <td>[2020-07-24]</td>\n",
       "      <td>danmark</td>\n",
       "      <td>[it]</td>\n",
       "      <td>it/itdrift</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>DPA-System søger en IT- og systemadministrator</td>\n",
       "      <td>VIGA Recruitment&lt;/b&gt;, &lt;b&gt;København</td>\n",
       "      <td>[2020-07-24]</td>\n",
       "      <td>danmark</td>\n",
       "      <td>[it]</td>\n",
       "      <td>it/itdrift</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>280 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Job Title  \\\n",
       "0                                Senior Data Scientist   \n",
       "1    Business Intelligence-konsulent til specialise...   \n",
       "2    Data warehouse-arkitekt med erfaring inden for...   \n",
       "3                       Business Intelligence Arkitekt   \n",
       "4              Business Intelligence Backend Developer   \n",
       "..                                                 ...   \n",
       "275          Data Center Operations Specialist at NNIT   \n",
       "276                                       It-konsulent   \n",
       "277   Shelter and Infrastructure Assistant (Technical)   \n",
       "278           Infrastructure and Operations Specialist   \n",
       "279     DPA-System søger en IT- og systemadministrator   \n",
       "\n",
       "                                   Company Publication Date Geographical Area  \\\n",
       "0                                   Ørsted     [2020-08-21]           danmark   \n",
       "1                             Jobindex A/S     [2020-08-19]           danmark   \n",
       "2                                 Nykredit     [2020-08-19]           danmark   \n",
       "3                           Systematic A/S     [2020-08-17]           danmark   \n",
       "4                           Systematic A/S     [2020-08-17]           danmark   \n",
       "..                                     ...              ...               ...   \n",
       "275                    NNIT</b>, <b>Søborg     [2020-07-24]           danmark   \n",
       "276             CESCOM IT A/S</b>, <b>Aars     [2020-07-24]           danmark   \n",
       "277  Dansk Flygtningehjælp</b>, <b>Danmark     [2020-07-24]           danmark   \n",
       "278                Enghouse</b>, <b>Aarhus     [2020-07-24]           danmark   \n",
       "279     VIGA Recruitment</b>, <b>København     [2020-07-24]           danmark   \n",
       "\n",
       "    Job Category Job Subcategory  \n",
       "0           [it]     it/database  \n",
       "1           [it]     it/database  \n",
       "2           [it]     it/database  \n",
       "3           [it]     it/database  \n",
       "4           [it]     it/database  \n",
       "..           ...             ...  \n",
       "275         [it]      it/itdrift  \n",
       "276         [it]      it/itdrift  \n",
       "277         [it]      it/itdrift  \n",
       "278         [it]      it/itdrift  \n",
       "279         [it]      it/itdrift  \n",
       "\n",
       "[280 rows x 6 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Make into pandas dataframe\n",
    "import pandas as pd\n",
    "filter_data = pd.DataFrame(filter_jobs)\n",
    "filter_data.columns = [\"Job Title\", \"Company\", \"Publication Date\", \"Geographical Area\", \"Job Category\", \"Job Subcategory\"]\n",
    "filter_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
