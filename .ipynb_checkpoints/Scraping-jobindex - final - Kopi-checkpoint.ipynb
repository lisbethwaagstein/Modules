{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exam project: The remote labour market in Denmark\n",
    "\n",
    "This notebook is used for webscraping www.jobindex.dk for job postings in the period 2010-2020. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "\n",
    "#Make log\n",
    "import scraping_class\n",
    "logfile = 'log_jobindex_scraping.txt'\n",
    "connector = scraping_class.Connector(logfile)\n",
    "\n",
    "#Define todays date and enddate for data\n",
    "from datetime import date\n",
    "today = date.today().strftime(\"%Y%m%d\") # to get format 20080101\n",
    "end_date = '20100101'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web-scraping job postings from Jobindex\n",
    "##### Geographical region and job category\n",
    "www.jobindex.dk/jobsoegning has filters for geographical area and job category, respectively. We will scrape this to be able to describe in which areas and industries remote working is especially taking place. \n",
    "The address of the company is also attached to the job posting, and even though this information would be more detailed, we have experienced that the address often belongs to the headquarter of the company and not the specific area, where the job is. Therefore we use the information from the geographical filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the basic url\n",
    "url = 'https://www.jobindex.dk/jobsoegning'\n",
    "response = requests.get(url)\n",
    "html = response.text\n",
    "soup = BeautifulSoup(html,'lxml')\n",
    "\n",
    "#Areas are given in the html by div id=areas and class=area_label\n",
    "areas_html = soup.find('div', {'id':'areas'})#('a', {'class':'area_label'})\n",
    "areas_div = str(areas_html).split('href=\"/jobsoegning/')[1:]\n",
    "\n",
    "#Find the names of each of the areas\n",
    "areas = () #we make a tuple, so we can order it\n",
    "areas_delete = ('skaane', 'faeroeerne', 'udlandet', 'groenland', 'danmark') #delete regions outside Denmark's borders\n",
    "for area in areas_div:\n",
    "    link = area.split('\"')[0]\n",
    "    if link in areas_delete:\n",
    "        del link\n",
    "    else:\n",
    "        areas += (link,)\n",
    "areas += ('danmark',) #make sure Danmark is the last element in the tuple. \n",
    "#print(areas)\n",
    "\n",
    "#Categories are given in the html by and id=categories\n",
    "cat_html = soup.find('div', {'id':'categories'})\n",
    "cat_div = soup.find_all('a',{'class':'filter-section-header title collapsed'}) #the filter collapses\n",
    "\n",
    "categories = []\n",
    "subcategories = []\n",
    "for cat in cat_div:\n",
    "    #first find the overall categories\n",
    "    cat_name = re.findall(r'(?<=<span>)(.*)(?=<span class)', str(cat))[0]\n",
    "    cat_name = cat_name.replace('\\xad','')\n",
    "    cat_links = re.findall(\"href=[\\\"\\'](.*?)[\\\"\\']\", str(cat))\n",
    "    cat_id = str(cat_links[0])\n",
    "    cat_id = cat_id.replace('#','')\n",
    "    category = [cat_name,cat_id]\n",
    "    categories.append(category)\n",
    "    #next find the sub-categories\n",
    "    subcat_div = str(cat_html).split('href=\"/jobsoegning/')[1:]\n",
    "for subcat in subcat_div:\n",
    "    subcategory = subcat.split('\"')[0]\n",
    "    subcategories.append(subcategory)\n",
    "#print(categories)\n",
    "#print(subcategories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Defining \"remote job postings\"\n",
    "We define a string of words and phrases related to working remotely to search through Jobindex for remote job postings. We include both Danish and English words/phrases, as we have experienced a lot of English job postings in bigger companies.\n",
    "We started out by trying to webscrape all job postings over the period and would have labeled remote/not remote afterwards, however there was approx. 3 million job postings and the scraping would take days. Instead we scrape the details of the relevant job postings and scrape the sum of the rest to be able to report relative numbers (although they are not perfect).\n",
    "\n",
    "##### Number of postings and pages\n",
    "Each page of www.jobindex.dk/jobsoegning contains 20 joblistings. Since the number of job postings differ from time to time, so does the number of pages. To be able to scrape job postings on all pages, we look at the pagination and find the max page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the search words in jobindex for working remotely\n",
    "remotely = ['hjemmekontor','hjemmearbejde','fjernarbejde','arbejde%2Bhjemmefra','arbejde%2Bhjemme','remote%2Barbejdsplads','remote%2Bwork','work%2Bremotely', 'working%2Bremotely','working%2Bfrom%2Bhome', 'home%2Boffice','home-office']\n",
    "\n",
    "#Scraping in loops from jobindex.\n",
    "jobs = []\n",
    "for area in areas:\n",
    "    for subcategory in subcategories:\n",
    "        print('Now requesting url: ' + subcategory +' and '+ area)\n",
    "        for remote in remotely:\n",
    "            url = f'https://www.jobindex.dk/jobsoegning/{subcategory}/{area}?jobage=archive&maxdate={today}&mindate={end_date}&q={remote}'\n",
    "            response = requests.get(url) \n",
    "            soup = BeautifulSoup(response.text,'lxml')\n",
    "        \n",
    "            #Identify total number of job postings in the given quarter\n",
    "            pages = BeautifulSoup(str(soup.find_all(\"div\", {'class':'jix_pagination_total'}))).get_text() \n",
    "            total = re.findall(r'af(?s)(.*)resultat',str(pages))\n",
    "            total = str(total).strip(\"[]\").strip(\"''\").strip().replace('.','')\n",
    "            if total == 0:\n",
    "                continue\n",
    "            #Use total number of job postings to calculate total number of pages\n",
    "            last_page = math.ceil(int(total)/20)\n",
    "        \n",
    "            #Loop through pages\n",
    "            for page in range(1,last_page+1):\n",
    "                if  page % 20 == 0:\n",
    "                    print('Now scraping page: ',page,'/', last_page, ' for ', subcategory, ' and ', area)\n",
    "                url = f'https://www.jobindex.dk/jobsoegning/{subcategory}/{area}?jobage=archive&maxdate={today}&mindate=20150101&page={page}&q={remote}'\n",
    "                response = requests.get(url)  \n",
    "                html = response.text   \n",
    "                if '<strong>' in html: \n",
    "                    html = html.replace('<strong>','<b>')\\\n",
    "                                .replace('</strong>','</b>')\n",
    "                else:\n",
    "                    print('No <strong>')\n",
    "                soup = BeautifulSoup(html,'lxml') \n",
    "                joblistings = soup.find_all('div',{'class':'jobsearch-result'}) \n",
    "                        \n",
    "                for joblisting in joblistings: #Loop through job listings\n",
    "                    #Find title\n",
    "                    title = re.findall(r'<b>(.*?)</b>', str(joblisting))[0]\n",
    "                    if 'amp;' in title:\n",
    "                        title = title.replace('amp;','')\n",
    "                    #Find company name\n",
    "                    if len(re.findall(r'<b>(.*?)</b>', str(joblisting)))>1:\n",
    "                        company = re.findall(r'<b>(.*?)</b>', str(joblisting))[1]\n",
    "                        if 'amp;' in company:\n",
    "                            company = company.replace('amp;','') \n",
    "                    #Find publication date\n",
    "                    pub_date = re.findall(r'time\\sdatetime=\"(.*)\"', str(joblisting))[0]\n",
    "                    geo_area = area #Save geographical area\n",
    "                    cat_id = re.findall('(.*?)/', str(subcategory))[0] #Save category \n",
    "                    subcat_id = subcategory #Save subcategory\n",
    "                    remote_word = remote\n",
    "                    #Find link to job posting\n",
    "                    descrip = BeautifulSoup(str(joblisting), 'lxml')\n",
    "                    for div in descrip.find_all(\"div\", {'class':'jix_toolbar jix_appetizer_toolbar'}): # remove toolbar at the end of each job\n",
    "                        div.decompose()\n",
    "                    for span in descrip.find_all(\"span\", {'class':'jix_toolbar jix_appetizer_toolbar'}): # remove toolbar at the end of each job\n",
    "                        span.decompose()\n",
    "                    links = re.findall(\"href=[\\\"\\'](.*?)[\\\"\\']\", str(descrip))\n",
    "                    if len(links)>1:\n",
    "                        link = str(links[1])\n",
    "                    else:\n",
    "                        link = str(links[0])\n",
    "                    link = link.replace(\"&amp;\", \"&\")\n",
    "                    # Indirectly extracting the short job description by removing everything else\n",
    "                    descrip = descrip.get_text()\n",
    "                    descrip = descrip.replace(\"\\n\", \" \") # remove \\n\n",
    "                    job = [title, company, pub_date, remote_word, geo_area, cat_id, subcat_id, link, descrip]\n",
    "                    jobs.append(job)\n",
    "    df = pd.DataFrame(jobs)\n",
    "    df.columns = ['Job Title','Company','Publication Date','Remote word','Geographical Area','Job Category','Job Subcategory', 'Link', 'Description']\n",
    "    df.to_pickle(r'C:\\Users\\miche\\Dropbox\\KU Introduction to Social Data Science\\Modules\\Jobs_basic_info')\n",
    "    print('Finished area: ',area)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f'https://www.jobindex.dk/jobsoegning/{subcategory}/{area}?jobage=archive&maxdate={today}&mindate=20150101&page={page}&q={remote}'\n",
    "response = requests.get(url)  \n",
    "html = response.text   \n",
    "if '<strong>' in html: \n",
    "html = html.replace('<strong>','<b>')\\\n",
    "                                .replace('</strong>','</b>')\n",
    "                else:\n",
    "                    print('No <strong>')\n",
    "                soup = BeautifulSoup(html,'lxml') \n",
    "                joblistings = soup.find_all('div',{'class':'jobsearch-result'}) \n",
    "                        \n",
    "                for joblisting in joblistings: #Loop through job listings\n",
    "                    #Find title\n",
    "                    title = re.findall(r'<b>(.*?)</b>', str(joblisting))[0]\n",
    "                    if 'amp;' in title:\n",
    "                        title = title.replace('amp;','')\n",
    "                    #Find company name\n",
    "                    if len(re.findall(r'<b>(.*?)</b>', str(joblisting)))>1:\n",
    "                        company = re.findall(r'<b>(.*?)</b>', str(joblisting))[1]\n",
    "                        if 'amp;' in company:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 0/40 job_links\n",
      "Completed 1/40 job_links\n",
      "Completed 2/40 job_links\n",
      "Completed 3/40 job_links\n",
      "Completed 4/40 job_links\n",
      "Completed 5/40 job_links\n",
      "Completed 6/40 job_links\n",
      "Completed 7/40 job_links\n",
      "Completed 8/40 job_links\n",
      "Completed 9/40 job_links\n",
      "Completed 10/40 job_links\n",
      "Completed 11/40 job_links\n",
      "Completed 12/40 job_links\n",
      "Completed 13/40 job_links\n",
      "Completed 14/40 job_links\n",
      "Completed 15/40 job_links\n",
      "Completed 16/40 job_links\n",
      "Completed 17/40 job_links\n",
      "Completed 18/40 job_links\n",
      "Completed 19/40 job_links\n",
      "Completed 20/40 job_links\n",
      "Completed 21/40 job_links\n",
      "Completed 22/40 job_links\n",
      "Completed 23/40 job_links\n",
      "Completed 24/40 job_links\n",
      "Completed 25/40 job_links\n",
      "Completed 26/40 job_links\n",
      "Completed 27/40 job_links\n",
      "Completed 28/40 job_links\n",
      "Completed 29/40 job_links\n",
      "Completed 30/40 job_links\n",
      "Completed 31/40 job_links\n",
      "Completed 32/40 job_links\n",
      "Completed 33/40 job_links\n",
      "Completed 34/40 job_links\n",
      "Completed 35/40 job_links\n",
      "Completed 36/40 job_links\n",
      "Completed 37/40 job_links\n",
      "Completed 38/40 job_links\n",
      "Completed 39/40 job_links\n"
     ]
    }
   ],
   "source": [
    "'''import PyPDF2 \n",
    "import textract\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import slate3k as slate\n",
    "import pandas as pd\n",
    "\n",
    "jobindex_links = []\n",
    "for i in range(1,3): # NOTE I'm only testing it on the first two pages. Use last_page when ready\n",
    "    url = f'https://www.jobindex.dk/jobsoegning?lang=da?page={i}.html'\n",
    "    jobindex_links.append(url)   \n",
    "#print(jobindex_links)\n",
    "html = []\n",
    "job_links = []\n",
    "jobs = []\n",
    "dates = []\n",
    "\n",
    "for url in jobindex_links:\n",
    "    response = requests.get(url)  \n",
    "    html = response.text  \n",
    "    soup = BeautifulSoup(html,'lxml')\n",
    "    #one job result is given by class=jobsearch-result\n",
    "    joblistings = soup.find_all('div',{'class':'jobsearch-result'})\n",
    "    #print(joblistings)\n",
    "    for joblisting in joblistings:\n",
    "        title = re.findall(r'(?<=<b>)(.*)(?=</b>)', str(joblisting))[0]\n",
    "        links = re.findall(\"href=[\\\"\\'](.*?)[\\\"\\']\", str(joblisting))\n",
    "        link = str(links[1])\n",
    "        link = link.replace(\"&amp;\", \"&\")\n",
    "        pub_date = re.findall(r'time\\sdatetime=\"(.*)\"', str(joblisting))\n",
    "        dates.append(pub_date) \n",
    "        job = [title,link,pub_date]\n",
    "        jobs.append(job) # Jobs is a list of list where each list contains the title job, and the link for that job, this will help later\n",
    "        job_links.append(link)\n",
    "\n",
    "for i in range(len(job_links)):\n",
    "    if 'pdf' in jobs[i][1]:\n",
    "        #scraping a PDF file\n",
    "        #request the url\n",
    "        url = jobs[i][1]\n",
    "        response = requests.get(url)\n",
    "        #save the pdf in the current folder\n",
    "        with open('pdf.pdf', 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        #open the pdf and get the text in doc\n",
    "        with open('pdf.pdf', 'rb') as fp:\n",
    "            doc = slate.PDF(fp)\n",
    "            doc = str(doc).replace('\\n','')\n",
    "            doc = str(doc).replace('\\r','')\n",
    "            jobs[i].append(doc)\n",
    "    else:\n",
    "        link = jobs[i][1]\n",
    "        response = requests.get(link)\n",
    "        html = response.text\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        description = soup.get_text()\n",
    "        description = description.replace('\\n','')\n",
    "        description = description.replace('\\r','')\n",
    "        jobs[i].append(description)\n",
    "    \n",
    "    print('Completed %d/%d job_links' % (i,len(job_links)))\n",
    "\n",
    "#transforming into dataframe\n",
    "df = pd.DataFrame(jobs)\n",
    "df.columns = [\"Job Title\", \"Link\",\"Date\", \"Description\",]'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the data\n",
    "We clean the data for duplicate observations from scraping url's, that may contain some of the same job postings. It is done in steps to be able to report the proces in our paper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset before removing duplicates:  35800\n",
      "Length after dropping all complete duplicates:  34395\n",
      "Length after dropping duplicates on remote word:  30774\n",
      "Length after dropping duplicates on category and subcategory:  22933\n",
      "Length after dropping duplicates on geographical area:  9371\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "unpickled_df = pd.read_pickle(r'C:\\Users\\miche\\Dropbox\\KU Introduction to Social Data Science\\Modules\\Jobs_basic_info18-20')\n",
    "print('Length of dataset before removing duplicates: ',len(unpickled_df))\n",
    "\n",
    "#Removing duplicated step by step\n",
    "#Dropping completely duplicte entries \n",
    "dupl1 = unpickled_df.drop_duplicates(keep = 'first')\n",
    "print('Length after dropping all complete duplicates: ', len(dupl1))\n",
    "\n",
    "#Drop duplicate entries based on all columns besides remote word (=> same job posting could occur in multiple searchings)\n",
    "dupl2 = dupl1.drop_duplicates(subset=['Job Title','Company','Publication Date','Geographical Area','Job Category','Job Subcategory','Link','Description'], keep = 'first')\n",
    "print('Length after dropping duplicates on remote word: ',len(dupl2))\n",
    "\n",
    "#Drop duplicate entries based on all columns besides category and subcategory (=> same job could occur in more categories)\n",
    "dupl3 = dupl2.drop_duplicates(subset=['Job Title','Company','Publication Date','Geographical Area','Link','Description'], keep = 'first')\n",
    "print('Length after dropping duplicates on category and subcategory: ',len(dupl3))\n",
    "\n",
    "#Drop duplicate entries based on all columns besides remote word and geographical area (=> same job could occur)\n",
    "#Be aware that the same job posting can be in more areas\n",
    "dupl4 = dupl3.drop_duplicates(subset=['Job Title','Company','Publication Date','Link','Description'], keep = 'first')\n",
    "print('Length after dropping duplicates on geographical area: ',len(dupl4))\n",
    "\n",
    "######FINAL DATA###########################################################################################\n",
    "final_data = dupl4\n",
    "final_data.to_pickle(r'C:\\Users\\miche\\Dropbox\\KU Introduction to Social Data Science\\Modules\\final_data.pkl')\n",
    "final_data.to_csv(r'C:\\Users\\miche\\Dropbox\\KU Introduction to Social Data Science\\Modules\\final_data.csv')\n",
    "############################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying jobs with possibility of working remotely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we define a list of keyword that we think a job posting where working remotely is a possibility will include\n",
    "\n",
    "keywords = ['hjemmefra', 'arbejde hjemme', 'hjemmearbejde','hjemmekontor', 'arbejde hjemmefra', 'arbejde remote', 'fjernarbejde']\n",
    "\n",
    "# I will now look for these words in the description column. I am not using the tokenized version of column because then I \n",
    "# can't look for expressions with more than one word. If you want to use the tokenized version then you have to use bigrams\n",
    "\n",
    "df['Remote'] = '0'\n",
    "for word in keywords:\n",
    "    for i in range(len(df)):\n",
    "        df['Description'][i] = str(df['Description'][i]).lower()\n",
    "        if word in df['Description'][i]:\n",
    "            df['Remote'][i] = '1'\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying the most popular words in job descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first step is to clean the job descriptions as much as possible so we can narrow down the words included for \n",
    "# analysis as much as possible\n",
    "\n",
    "df['Tokenized_description'] = ' '\n",
    "symbols = ['?','!','>','<','-','[',']','(',')','{','}',' –','``',\"''\",'\"\"','\\\\','@','$','&','=']\n",
    "for i in range(len(df)):\n",
    "    stop_words = nltk.corpus.stopwords.words(\"danish\")\n",
    "    df['Tokenized_description'][i] = re.sub(r'(\\.+ )|,|\\||:|/|\\'|\\-|;|\\*|!|(\\s\\d+\\s)|(\\s\\W\\s)',' ',str(df['Description'][i]))\n",
    "    df['Tokenized_description'][i] = str(df['Tokenized_description'][i]).rstrip('\\\\')\n",
    "    df['Tokenized_description'][i] = nltk.word_tokenize(str(df['Tokenized_description'][i].lower()))\n",
    "    df['Tokenized_description'][i] = [w for w in df['Tokenized_description'][i] if not w in stop_words] \n",
    "    df['Tokenized_description'][i] = [w for w in df['Tokenized_description'][i] if not w in symbols]\n",
    "    df['Tokenized_description'][i] = [w for w in df['Tokenized_description'][i] if not w.isnumeric()]\n",
    "    df['Tokenized_description'][i] = [word for word in df['Tokenized_description'][i] if len(word) > 3]\n",
    "    \n",
    "# Finding intersections --- USELESS\n",
    "df['description_set'] = df['Tokenized_description'].apply(set)\n",
    "description_set = df['description_set'].tolist() #convert each description into a set \n",
    "set.intersection(*description_set)\n",
    "del df['description_set']\n",
    "\n",
    "# Finding most frequent words in all descriptions --- USELESS ATM BUT MAYBE IF WE CLEAN IT ENOUGH IT WILL WORK\n",
    "\n",
    "descriptions_list = [] \n",
    "# This loop will pull all tokens in one bag\n",
    "for i in range(len(df)):\n",
    "    descriptions_list.extend(df['Tokenized_description'][i]) \n",
    "    \n",
    "word_dist = nltk.FreqDist(descriptions_list) word_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL DEDICATED TO KEEP CLEANING DESCRIPTIONS\n",
    "\n",
    "def clean(doc):\n",
    "    #doc = doc.replace('\\n','')\n",
    "    #doc = doc.replace('\\r','')\n",
    "    doc = doc.replace('\\t','')\n",
    "    doc = doc.replace('\\'','')\n",
    "    doc = doc.replace('|','')\n",
    "    doc = doc.replace('/','')\n",
    "\n",
    "def strip_html(row):\n",
    "    return str(html.fromstring(row).text_content())\n",
    "\n",
    "from lxml import html\n",
    "for row in df['Description']:\n",
    "    strip_html(str(row))\n",
    "    \n",
    "    \n",
    "    \n",
    "#text = re.sub('<[^>]*>', '', text)\n",
    "#emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assigning DISCO-08 codes to the job postings\n",
    "This function will be used to assign a DISCO-08 code to the job postings. It goes through the following steps:\n",
    "\n",
    "1. Importing a csv file downloaded from Danmarks Statistik which includes the different job functions that fall under each DISCO-08 category\n",
    "2. Clean this csv file and converting it into a dataframe with two columns, one that has the disco codes and one that has a list of all the jobs that fall into each code\n",
    "3. Tokenizing the jobs \n",
    "4. Tokenizing the words in the 'Job Title' column from our job postings data\n",
    "5. Creating a function that compares the tokens in the job titles to the tokens in the DISCO dataframe and if a match is found, then returning the corresponding DISCO code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lisbe\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "# Importing and cleaning DISCO-08 classification\n",
    "import pandas as pd\n",
    "import re\n",
    "disco = pd.read_csv(r\"C:\\Users\\lisbe\\OneDrive\\11. Semester\\Social Data Science\\Modules\\csv_da.csv\",header=None)\n",
    "disco_clean = disco.copy()\n",
    "\n",
    "# Remove words that start with lowercase\n",
    "for i in range(len(disco_clean)):\n",
    "    disco_clean[3][i] = ' '.join([word for word in str(disco_clean[3][i]).split(' ') if not word.islower()])\n",
    "\n",
    "# Create a dictionary for DISCO functions\n",
    "\n",
    "disco_clean['DISCO'] = disco_clean[0].astype(str).str[:1] #This column will have the highest hierarchy code 0-9\n",
    "del disco_clean[0]\n",
    "del disco_clean[1]\n",
    "del disco_clean[2]\n",
    "disco_clean = disco_clean.groupby(by=disco_clean['DISCO']).sum()\n",
    "disco_clean.rename({3:'functions'}, axis='columns', inplace = True) # rename column \n",
    "for i in range(9): # Adding a space in between words that are missing them \"LikeThis\"\n",
    "    disco_clean['DISCO'] = disco_clean['DISCO'].astype(str).str[:1] # We only want the first number \n",
    "    disco_clean['functions'][i] = re.sub(r\"(\\w)([A-Z])\", r\"\\1 \\2\", str(disco_clean['functions'][i]))\n",
    "    \n",
    "disco_clean[\"functions_tokenized\"] = \"\"\n",
    "for i in range(len(disco_clean)):\n",
    "    disco_clean['functions_tokenized'][i]=nltk.word_tokenize(str(disco_clean['functions'][i]).lower())\n",
    "    \n",
    "# Now I would like a job type column from the job titles where I would extract the nouns from job titles\n",
    "#pip install afinn\n",
    "import nltk\n",
    "df['Job Title']=df['Job Title'].str.replace(',','')\n",
    "df[\"tokenized_titles\"] = \"\"\n",
    "for i in range(len(df)):\n",
    "    df['tokenized_titles'][i] = nltk.word_tokenize(str(df['Job Title'][i]).lower())\n",
    "\n",
    "# This loop creates a new column in the df that assigns a disco code to each posting based on the words in the title\n",
    "\n",
    "df['disco'] = ''\n",
    "for m in range(len(df)):\n",
    "    for i in range(len(disco_clean)):\n",
    "        for element in disco_clean['functions_tokenized'][i]:\n",
    "            if element in df['tokenized_titles'][m]:\n",
    "                df['disco'][m] = disco_clean['DISCO'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
