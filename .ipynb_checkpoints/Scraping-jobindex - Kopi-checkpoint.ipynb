{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exam: Scraping job postings from Jobindex\n",
    "\n",
    "...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scraping_class\n",
    "logfile = 'log_jobindex_scraping.txt'\n",
    "connector = scraping_class.Connector(logfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "\n",
    "#define url and fetch the HTML using the requests module\n",
    "url = 'https://www.jobindex.dk/jobsoegning?lang=da.html'\n",
    "response = requests.get(url)  \n",
    "html = response.text  \n",
    "soup = BeautifulSoup(html,'lxml') # parse the raw html using BeautifoulSoup\n",
    "#print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "649\n"
     ]
    }
   ],
   "source": [
    "#find the max page number\n",
    "max_page = soup.find('ul',{'class':'pagination'})\n",
    "max_page2 = max_page.find_all('a', {'class':'page-link'})\n",
    "\n",
    "def convert_value_type(value_node):\n",
    "    if value_node.name == 'a':\n",
    "        return value_node.text\n",
    "\n",
    "page_list = []\n",
    "for page in max_page2:\n",
    "    page_list.append(convert_value_type(page))\n",
    "#print(page_list[-1])\n",
    "\n",
    "last_page = int(page_list[-1]) + 1 # we add one to use it in range in for-loop\n",
    "print(last_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-155-c33f420a3eef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[0mall_links\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlinks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[0mnew\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlink\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\"'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m     \u001b[0mprin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: tuple indices must be integers or slices, not list"
     ]
    }
   ],
   "source": [
    "#check exercise 6.1.4??\n",
    "#Get all link pages from jobindex\n",
    "jobindex_links = []\n",
    "for i in range(1,last_page):\n",
    "    url = f'https://www.jobindex.dk/jobsoegning?lang=da?page={i}.html'\n",
    "    jobindex_links.append(url)   \n",
    "#print(jobindex_links)\n",
    "\n",
    "#Extract data from one page link first. We want the\n",
    "url = 'https://www.jobindex.dk/jobsoegning?lang=da?page=1.html'\n",
    "response = requests.get(url)  \n",
    "html = response.text  \n",
    "soup = BeautifulSoup(html,'lxml')\n",
    "\n",
    "#one job result is given by class=jobsearch-result\n",
    "joblistings = soup.find_all('div',{'class':'jobsearch-result'})\n",
    "#print(joblistings)\n",
    "\n",
    "# now find the a href hyperlinks of the individual jos postings. note: postings have many hyperlinks.\n",
    "#first divide into postings in a list\n",
    "postings = []\n",
    "for posting_loc in html.split('<div class=\"jobsearch-result\">')[1:]:\n",
    "    posting = posting_loc.split('</div>')[0].lower()\n",
    "    postings.append(posting)\n",
    "#postings_join ='NEWJOB'.join(postings)\n",
    "    #postings += str(posting)\n",
    "#print(postings)\n",
    "\n",
    "#now find all hyperlinks within a posting\n",
    "links = []\n",
    "for posting in postings:\n",
    "    link_loc = posting.split('href=\"')[1:]\n",
    "    links.append(link_loc) \n",
    "#print(len(links)) # there is 20 postings per page\n",
    "#print(links[19][1]) # this is where the link to the job posting is\n",
    "\n",
    "all_links = []\n",
    "for i in links:\n",
    "    new = link[i][1].split('\"')[0]\n",
    "    prin(new)\n",
    "    \n",
    "   \n",
    "'''    \n",
    "    for link in link_loc:\n",
    "    second_link = link[.split('\"')[0]\n",
    "    links.append(second_link)\n",
    "print(links)\n",
    "#for link in links:\n",
    "#    link_done = link.split('\"')[0]\n",
    "#    all_links.append(link_done)\n",
    "#print(all_links) \n",
    " #   link = link.split('\"')[0]\n",
    "'''\n",
    "'''for link_loc in postings_join.split('href=\"')[1:]:\n",
    "    link = link_loc.split('\"')[0]\n",
    "    links.add(link)\n",
    "print(links)'''\n",
    "\n",
    "'''for link_loc in \n",
    "    link = link_loc.split('\"')[0]\n",
    "    if '/categories/' in link:\n",
    "        links.add(link)\n",
    "print(len(links),list(links)[0]) # link is relative\n",
    "links = ['https://www.trustpilot.com'+link for link in links]# add the domain to each link\n",
    "links[:10]\n",
    "'''\n",
    "'''\n",
    "url = 'https://www.jobindex.dk/jobsoegning?lang=da?page=1.html'\n",
    "response, call_id = connector.get(url,'mapping_categories')\n",
    "if response.ok:\n",
    "    html = response.text\n",
    "else:\n",
    "    print('error')\n",
    "'''                                      \n",
    "'''\n",
    "#Extract data from all the links\n",
    "jobindex_data = []\n",
    "for link in jobindex_links:\n",
    "    response = requests.get(link)  \n",
    "    html = response.text  \n",
    "    soup = BeautifulSoup(html,'lxml') # parse the raw html using BeautifoulSoup\n",
    "    \n",
    "    #print(soup) '''   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
