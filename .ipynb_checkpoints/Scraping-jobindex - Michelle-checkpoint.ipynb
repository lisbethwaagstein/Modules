{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exam: Scraping job postings from Jobindex\n",
    "\n",
    "...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scraping_class\n",
    "logfile = 'log_jobindex_scraping.txt'\n",
    "connector = scraping_class.Connector(logfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "\n",
    "#define url and fetch the HTML using the requests module\n",
    "url = 'https://www.jobindex.dk/jobsoegning?lang=da.html'\n",
    "response = requests.get(url)  \n",
    "html = response.text  \n",
    "soup = BeautifulSoup(html,'lxml') # parse the raw html using BeautifoulSoup\n",
    "#print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "649\n"
     ]
    }
   ],
   "source": [
    "#find the max page number\n",
    "max_page = soup.find('ul',{'class':'pagination'})\n",
    "max_page2 = max_page.find_all('a', {'class':'page-link'})\n",
    "\n",
    "def convert_value_type(value_node):\n",
    "    if value_node.name == 'a':\n",
    "        return value_node.text\n",
    "\n",
    "page_list = []\n",
    "for page in max_page2:\n",
    "    page_list.append(convert_value_type(page))\n",
    "#print(page_list[-1])\n",
    "\n",
    "last_page = int(page_list[-1]) + 1 # we add one to use it in range in for-loop\n",
    "print(last_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "https://stormgroup.recruitee.com/o/salgskonsulent-til-sjov-social-hverdag\" data-click=\"/c?t=h989946&amp;ctx=w&amp;u=20313204\" rel=\"noopener\" target=\"_blank\"><b>salgskonsulent til sjov, social hverdag</b></a>\n",
      "<p>\n",
      "<a \n",
      "https://ikast-brande.emply.net/recruitment/vacancyad.aspx?publishingid=1fb545a3-1946-4afc-8484-9c336901cb5b\n",
      "https://ikast-brande.emply.net/recruitment/vacancyad.aspx?publishingid=5346be46-89e6-4202-aa7e-5f497d5ada6b\n",
      "https://www.jobindex.dk/jobannonce/366043/socialfaglig-medarbejder\n",
      "https://www.birn-partners.com/jobs/?hr=show-job%2f59547&amp;locale=da_dk\n",
      "https://www.jobindex.dk/jobannonce/366045/udekoerende-daekmontoer-til-alsidigt-arbejde\n",
      "https://www.innomate.com/innomatepublicpagesmedarb/jobnotice.aspx?companyid=zbc&amp;jobnoticeid=477\n",
      "https://www.innomate.com/innomatepublicpagesmedarb/jobnotice.aspx?companyid=zbc&amp;jobnoticeid=479\n",
      "https://candidate.hr-manager.net/applicationinit.aspx?cid=5001&amp;projectid=134384&amp;departmentid=7957&amp;mediaid=5&amp;skipadvertisement=false\n",
      "https://www.odense.dk/job?vacancyid=27322\n",
      "https://www.innomate.com/innomatepublicpagesmedarb/jobnotice.aspx?companyid=sde&amp;jobnoticeid=1176\n",
      "https://candidate.hr-manager.net/applicationinit.aspx?cid=1627&amp;projectid=143600&amp;departmentid=18956&amp;mediaid=4619\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-242-af90b78b6159>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;31m#for link in links[i][1]:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\"'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m    \u001b[1;31m# all_links.add(split('\"')[0]) # this is where the link to the job posting is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "#check exercise 6.1.4??\n",
    "#Get all link pages from jobindex\n",
    "jobindex_links = []\n",
    "for i in range(1,last_page):\n",
    "    url = f'https://www.jobindex.dk/jobsoegning?lang=da?page={i}.html'\n",
    "    jobindex_links.append(url)   \n",
    "#print(jobindex_links)\n",
    "\n",
    "#Extract data from one page link first. We want the\n",
    "url = 'https://www.jobindex.dk/jobsoegning?lang=da?page=1.html'\n",
    "response = requests.get(url)  \n",
    "html = response.text  \n",
    "soup = BeautifulSoup(html,'lxml')\n",
    "\n",
    "#one job result is given by class=jobsearch-result\n",
    "joblistings = soup.find_all('div',{'class':'jobsearch-result'})\n",
    "#print(joblistings)\n",
    "\n",
    "# now find the a href hyperlinks of the individual jos postings. note: postings have many hyperlinks.\n",
    "#first divide into postings in a list\n",
    "postings = []\n",
    "for posting_loc in html.split('<div class=\"jobsearch-result\">')[1:]:\n",
    "    posting = posting_loc.split('</div>')[0].lower()\n",
    "    postings.append(posting)\n",
    "#print(postings)\n",
    "\n",
    "#now find all hyperlinks within a posting\n",
    "links = []\n",
    "for posting in postings:\n",
    "    link_loc = tuple(posting.split('href=\"')[1:])    #this is made into a tuple to be able to call the second element.\n",
    "    #link_loc2 = link_loc.split('\"')[0]\n",
    "    links.append(link_loc) \n",
    "print(len(links)) # there is 20 postings per page\n",
    "#print(links)\n",
    "\n",
    "#want to get the second element in all the \"links\" such as:\n",
    "#print(links[1][1])\n",
    "#print(links[2][1])\n",
    "print(links[14][1])\n",
    "\n",
    "all_links = []\n",
    "for i in range(1,len(links)):\n",
    "    #for link in links[i][1]:\n",
    "    print(links[i][1].split('\"')[0])\n",
    "        \n",
    "   # all_links.add(split('\"')[0]) # this is where the link to the job posting is\n",
    "\n",
    "#we have to replace \"amp\" in all links with \"\"\n",
    "   \n",
    "'''    \n",
    "    for link in link_loc:\n",
    "    second_link = link[.split('\"')[0]\n",
    "    links.append(second_link)\n",
    "print(links)\n",
    "#for link in links:\n",
    "#    link_done = link.split('\"')[0]\n",
    "#    all_links.append(link_done)\n",
    "#print(all_links) \n",
    " #   link = link.split('\"')[0]\n",
    "'''\n",
    "'''for link_loc in postings_join.split('href=\"')[1:]:\n",
    "    link = link_loc.split('\"')[0]\n",
    "    links.add(link)\n",
    "print(links)'''\n",
    "\n",
    "'''for link_loc in \n",
    "    link = link_loc.split('\"')[0]\n",
    "    if '/categories/' in link:\n",
    "        links.add(link)\n",
    "print(len(links),list(links)[0]) # link is relative\n",
    "links = ['https://www.trustpilot.com'+link for link in links]# add the domain to each link\n",
    "links[:10]\n",
    "'''\n",
    "'''\n",
    "url = 'https://www.jobindex.dk/jobsoegning?lang=da?page=1.html'\n",
    "response, call_id = connector.get(url,'mapping_categories')\n",
    "if response.ok:\n",
    "    html = response.text\n",
    "else:\n",
    "    print('error')\n",
    "'''                                      \n",
    "'''\n",
    "#Extract data from all the links\n",
    "jobindex_data = []\n",
    "for link in jobindex_links:\n",
    "    response = requests.get(link)  \n",
    "    html = response.text  \n",
    "    soup = BeautifulSoup(html,'lxml') # parse the raw html using BeautifoulSoup\n",
    "    \n",
    "    #print(soup) '''   \n",
    "https://candidate.hr-manager.net/ApplicationInit.aspx?cid=1627&ProjectId=143600&DepartmentId=18956&MediaId=4619"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting slate3kNote: you may need to restart the kernel to use updated packages.\n",
      "  Downloading slate3k-0.5.3-py2.py3-none-any.whl (7.9 kB)\n",
      "Collecting pdfminer3k\n",
      "  Downloading pdfminer3k-1.3.4-py3-none-any.whl (100 kB)\n",
      "Requirement already satisfied: ply in c:\\users\\miche\\anaconda3\\lib\\site-packages (from pdfminer3k->slate3k) (3.11)\n",
      "Installing collected packages: pdfminer3k, slate3k\n",
      "Successfully installed pdfminer3k-1.3.4 slate3k-0.5.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#pip install slate3k\n",
    "#pip install PyPDF2\n",
    "#pip install textract\n",
    "#pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Cannot locate objid=59\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import slate3k as slate\n",
    "#request the url\n",
    "url = 'https://www.jobindex.dk/img/pdf/LRS210820SOC.pdf'\n",
    "response = requests.get(url)\n",
    "\n",
    "#save the pdf in the current folder\n",
    "with open('pdf.pdf', 'wb') as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "#open the pdf and get the text in doc\n",
    "with open('pdf.pdf', 'rb') as fp:\n",
    "    doc = slate.PDF(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GolfBox A/S blev stiftet i 2003 og er siden blevet Skandinaviens største leverandør af administrationssoftware til golfklubber. GolfBox A/S™ software er 100% internetbaseret, hvilket bl.a. har medvirket til et stort teknologisk forspring i branchen globalt www.golfbox.netco@golfbox.dk. Sammen HenvendelseVi er i øjeblikket 19 engagerede medarbejdere hos GolfBox A/S.GolfBox A/S søger ambitiøs Programmør / Datamatiker+45 2173 4000 info@GolfBox.dk www.GolfBox.netGolfBox A/S Mere tid til golf...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n#while loop reads each page and extracts the text\\nwhile count < num_pages:\\n    pageObj = pdfReader.getPage(count)\\n    count +=1\\n    text += pageObj.extractText()\\n    #pdf_text = str(text).replace('\\n','')\\nprint(text)\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import PyPDF2 \n",
    "import textract\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#scraping a PDF file*\n",
    "filename = 'pdf.pdf' \n",
    "\n",
    "#open allows you to read the file.\n",
    "pdfFileObj = open(filename,'rb')\n",
    "\n",
    "#The pdfReader variable is a readable object that will be parsed.\n",
    "pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "\n",
    "#Discerning the number of pages will allow us to parse through all the pages.\n",
    "num_pages = pdfReader.numPages\n",
    "count = 0\n",
    "text = \"\"\n",
    "\n",
    "#while loop reads each page and extracts the text\n",
    "while count < num_pages:\n",
    "    pageObj = pdfReader.getPage(count)\n",
    "    count +=1\n",
    "    text += pageObj.extractText()\n",
    "    #pdf_text = str(text).replace('\\n','')\n",
    "print(text)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Geographical Area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JYSK Danmark søger Lageransvarlig til JYSK i R...</td>\n",
       "      <td>JYSK</td>\n",
       "      <td>bornholm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Centerchef i Børn og Familie</td>\n",
       "      <td>KL - Konsulentvirksomhed</td>\n",
       "      <td>bornholm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Myndighedsleder i Børn og Familie</td>\n",
       "      <td>KL - Konsulentvirksomhed</td>\n",
       "      <td>bornholm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Administrationschef</td>\n",
       "      <td>Bornholms Politi</td>\n",
       "      <td>bornholm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Vi har et bredt udvalg i IT-kurser, der udbyde...</td>\n",
       "      <td>Jobindex Kurser</td>\n",
       "      <td>bornholm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>Poolia IT&lt;/b&gt;, &lt;b&gt;Skåne, Sverige</td>\n",
       "      <td></td>\n",
       "      <td>skaane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>RecNet AB&lt;/b&gt;, &lt;b&gt;Skåne, Sverige</td>\n",
       "      <td></td>\n",
       "      <td>skaane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>RecNet AB&lt;/b&gt;, &lt;b&gt;Skåne, Sverige</td>\n",
       "      <td></td>\n",
       "      <td>skaane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>Trafikverket&lt;/b&gt;, &lt;b&gt;Skåne, Sverige</td>\n",
       "      <td></td>\n",
       "      <td>skaane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>Skåne, Sverige</td>\n",
       "      <td></td>\n",
       "      <td>skaane</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>243 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Job Title  \\\n",
       "0    JYSK Danmark søger Lageransvarlig til JYSK i R...   \n",
       "1                         Centerchef i Børn og Familie   \n",
       "2                    Myndighedsleder i Børn og Familie   \n",
       "3                                  Administrationschef   \n",
       "4    Vi har et bredt udvalg i IT-kurser, der udbyde...   \n",
       "..                                                 ...   \n",
       "238                   Poolia IT</b>, <b>Skåne, Sverige   \n",
       "239                   RecNet AB</b>, <b>Skåne, Sverige   \n",
       "240                   RecNet AB</b>, <b>Skåne, Sverige   \n",
       "241                Trafikverket</b>, <b>Skåne, Sverige   \n",
       "242                                     Skåne, Sverige   \n",
       "\n",
       "                      Company Geographical Area  \n",
       "0                        JYSK          bornholm  \n",
       "1    KL - Konsulentvirksomhed          bornholm  \n",
       "2    KL - Konsulentvirksomhed          bornholm  \n",
       "3            Bornholms Politi          bornholm  \n",
       "4             Jobindex Kurser          bornholm  \n",
       "..                        ...               ...  \n",
       "238                                      skaane  \n",
       "239                                      skaane  \n",
       "240                                      skaane  \n",
       "241                                      skaane  \n",
       "242                                      skaane  \n",
       "\n",
       "[243 rows x 3 columns]"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "#Define all the urls for the different geographical areas\n",
    "url = 'https://www.jobindex.dk/jobsoegning'\n",
    "response = requests.get(url)\n",
    "html = response.text\n",
    "soup = BeautifulSoup(html,'lxml')\n",
    "\n",
    "#Areas are given in the html by div id=areas and class=area_label\n",
    "areas_html = soup.find('div', {'id':'areas'})#('a', {'class':'area_label'})\n",
    "areas_div = str(areas_html).split('href=\"/jobsoegning/')[1:]\n",
    "\n",
    "#Find the links to each of the areas\n",
    "areas = set()\n",
    "for area in areas_div:\n",
    "    link = area.split('\"')[0]\n",
    "    areas.add(link)\n",
    "\n",
    "#Scraping the geographical areas from jobindex\n",
    "#the links to scrape are in the area_links list, so we want to loop over this one.\n",
    "area_data = []\n",
    "area_jobs = []\n",
    "\n",
    "#for page in range(1,last_page+1) SEE if it's better what they have done in #[Answer to Ex.6.2.3-4]!!!?\n",
    "for area in areas:\n",
    "    url = f'https://www.jobindex.dk/jobsoegning/{area}?page=1' # make into max page at some point\n",
    "    response = requests.get(url)  \n",
    "    html = response.text  \n",
    "    soup = BeautifulSoup(html,'lxml')\n",
    "    joblistings = soup.find_all('div',{'class':'jobsearch-result'})\n",
    "    #area_data.append(joblistings) \n",
    "    for joblisting in joblistings:\n",
    "        title = re.findall(r'(?<=<b>)(.*)(?=</b>)', str(joblisting))[0]\n",
    "        #print(len(re.findall(r'(?<=<b>)(.*)(?=</b>)', str(joblisting))))\n",
    "        if len(re.findall(r'(?<=<b>)(.*)(?=</b>)', str(joblisting)))>1:\n",
    "            company = re.findall(r'(?<=<b>)(.*)(?=</b>)', str(joblisting))[1]\n",
    "        else:\n",
    "            company = ''\n",
    "        geo_area = area\n",
    "        job = [title, company, geo_area]\n",
    "        area_jobs.append(job)\n",
    "        \n",
    "#Make into dataframe\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(area_jobs)\n",
    "df.columns = [\"Job Title\", \"Company\", \"Geographical Area\"]\n",
    "df\n",
    "#tjek dubletter på title!!\n",
    "#SKÅNE (måske al udlandet) har \"<strong>\" før titel og ikke <b>... skal de fjernes!!!?\n",
    "\n",
    "'''#find the max page for all the areas\n",
    "page_links = [link for link in links if '?page=' in link] # check if the paging parameter is in the link\n",
    "if len(page_links)==0: # no pages.\n",
    "    return [category_link]\n",
    "n_pages = max([int(link.split('page=')[-1]) for link in page_links]) # extract the page value and take the max\n",
    "paging_links = [category_link] # define container and store the original result page\n",
    "q = category_link+'?page=%d' # define the varying parameter string.\n",
    "for num in range(2,n_pages+1): # build the links.\n",
    "    paging_links.append(q%num)\n",
    "return paging_links\n",
    "max = paging_links\n",
    "print(max)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define all the urls for the different CATEGORIES\n",
    "'''url = 'https://www.jobindex.dk/jobsoegning'\n",
    "response = requests.get(url)\n",
    "html = response.text\n",
    "soup = BeautifulSoup(html,'lxml')\n",
    "\n",
    "#Areas are given in the html by div id=areas and class=area_label\n",
    "areas = soup.find('div', {'id':'areas'})#('a', {'class':'area_label'})\n",
    "areas_div = str(areas).split('href=\"')[1:]\n",
    "\n",
    "#Find the links to each of the areas\n",
    "links = set()\n",
    "for area in areas_div:\n",
    "    link = area.split('\"')[0]\n",
    "    if '/jobsoegning/' in link:\n",
    "        links.add(link)\n",
    "    #print(len(area_links),list(area_links)[0]) # link is relative\n",
    "    area_links = ['https://www.jobindex.dk'+link for link in links] #add the domain to each link\n",
    "print(area_links)\n",
    "\n",
    "#scraping the geographical areas from jobindex'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
