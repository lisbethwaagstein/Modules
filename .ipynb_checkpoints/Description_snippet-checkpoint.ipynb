{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "650\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import scraping_class\n",
    "logfile = 'log_jobindex_scraping.txt'\n",
    "connector = scraping_class.Connector(logfile)\n",
    "\n",
    "#Define the url and fetch the HTML using the requests module\n",
    "url = 'https://www.jobindex.dk/jobsoegning?lang=da.html'\n",
    "response = requests.get(url)  \n",
    "html = response.text  \n",
    "soup = BeautifulSoup(html,'lxml') # parse the raw html using BeautifoulSoup\n",
    "\n",
    "#Find the pagination and page links\n",
    "pagination = soup\\\n",
    "            .find('ul',{'class':'pagination'})\\\n",
    "            .find_all('a', {'class':'page-link'})\n",
    "#print(pagination)\n",
    "\n",
    "#Extract the text-part of the pagination links, i.e. not the link but the page number. \n",
    "page_list = []\n",
    "for page in pagination:\n",
    "    page_list.append(page.text)\n",
    "    last_page = int(page_list[-1]) #Save the last page number as \"last page\"\n",
    "print(last_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all link pages from jobindex\n",
    "jobindex_links = []\n",
    "title_company = []\n",
    "job_data = []\n",
    "for i in range(1,3):\n",
    "    url = f'https://www.jobindex.dk/jobsoegning?maxdate=20200826&mindate=20200526&jobage=archive'\n",
    "    jobindex_links.append(url)   \n",
    "# Get html behind each search page from jobindex\n",
    "for link in jobindex_links:\n",
    "    response,call_id = connector.get(link,'searchpage')\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    joblistings = soup.find_all('div',{'class':'jobsearch-result'})\n",
    "# Extract info for each job listing from each searchpage    \n",
    "    for joblisting in joblistings:\n",
    "        # find the job title and company which are in bold\n",
    "        title_and_company = re.findall(r'(?<=<b>)(.*)(?=</b>)', str(joblisting)) \n",
    "        title_and_company = title_and_company[0:2]\n",
    "        soup = BeautifulSoup(str(joblisting), 'lxml')\n",
    "        # Indirectly extracting the short job description by removing everything else\n",
    "        for div in soup.find_all(\"div\", {'class':'jix_toolbar jix_appetizer_toolbar'}): # remove toolbar at the end of each job\n",
    "            div.decompose()\n",
    "        soup = soup.get_text()\n",
    "        soup = soup.replace(\"\\n\", \" \") # remove \\n \n",
    "        job_info = [str(title_and_company[0]),str(title_and_company[1]),str(soup)] #create a list containing job title, company and description\n",
    "        job_data.append(job_info)\n",
    "    \n",
    "#transforming into dataframe\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(job_data)\n",
    "df.columns = [\"Job Title\", \"Company\", \"Description\"]\n",
    "\n",
    "\n",
    "# remove job title and company info from description column\n",
    "df['Description'] = df['Description'].replace(to_replace=r'\\b'+df['Job Title']+r'\\b', value='',regex=True)\n",
    "df['Description'] = df['Description'].replace(to_replace=r'\\b'+df['Company']+r'\\b', value='',regex=True)\n",
    "df['Description'] = df['Description'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('vores', 40),\n",
       " ('søger', 26),\n",
       " ('arbejde', 22),\n",
       " ('samt', 18),\n",
       " ('opgaver', 18),\n",
       " ('samarbejde', 14),\n",
       " ('børn', 14),\n",
       " ('stilling', 14),\n",
       " ('udvikling', 12),\n",
       " ('derfor', 12)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "df['Tokenized_description'] = ' '\n",
    "symbols = ['?','!','>','<','-','[',']','(',')','{','}',' –','``',\"''\",'\"\"','\\\\','@','$','&','=']\n",
    "for i in range(len(df)):\n",
    "    stop_words = nltk.corpus.stopwords.words(\"danish\")\n",
    "    df['Tokenized_description'][i] = re.sub(r'(\\.+ )|,|\\||:|/|\\'|\\-|;|\\*|!|(\\s\\d+\\s)|(\\s\\W\\s)',' ',str(df['Description'][i]))\n",
    "    df['Tokenized_description'][i] = str(df['Tokenized_description'][i]).rstrip('\\\\')\n",
    "    df['Tokenized_description'][i] = nltk.word_tokenize(str(df['Tokenized_description'][i].lower()))\n",
    "    df['Tokenized_description'][i] = [w for w in df['Tokenized_description'][i] if not w in stop_words] \n",
    "    df['Tokenized_description'][i] = [w for w in df['Tokenized_description'][i] if not w in symbols]\n",
    "    df['Tokenized_description'][i] = [w for w in df['Tokenized_description'][i] if not w.isnumeric()]\n",
    "    df['Tokenized_description'][i] = [word for word in df['Tokenized_description'][i] if len(word) > 3]\n",
    "    \n",
    "descriptions_list = [] \n",
    "# This loop will pull all tokens in one bag\n",
    "for i in range(len(df)):\n",
    "    descriptions_list.extend(df['Tokenized_description'][i]) \n",
    "    \n",
    "word_dist = nltk.FreqDist(descriptions_list)\n",
    "word_dist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
