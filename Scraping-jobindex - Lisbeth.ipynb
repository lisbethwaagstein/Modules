{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exam project: Scraping job postings from Jobindex\n",
    "\n",
    "This notebook is used for webscraping www.jobindex.dk for all job postings in the period xxxx-2020. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import scraping_class\n",
    "logfile = 'log_jobindex_scraping.txt'\n",
    "connector = scraping_class.Connector(logfile)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAABHCAYAAADvAPinAAATLUlEQVR4Ae1d4WtUVxbPf5Nv+bR+ST8GCvWT4gcZWCawoUFZsdAoW2mxkLUQERQh4IpgQCJLsViK2e0aEF02kMVFgmzcFtEqG2upLluzkjSJqWe5M+8mM5P35vfuzL057775BYbJvN9995zfOefd+c2d++70ra6uCh+MAWuANcAaYA2wBlgDvVQDfT///LPwwRiwBlgDrAHWAGuANdBLNdC3trYmfDAGrAHWAGuANcAaYA30Ug30ra+vCx+MAWuANcAaYA2wBlgDvVQDFEAUgBTArAHWAGuANcAa6Lka6NvY2BA+GAPWAGuANcAaYA2wBnqpBvrevHkjfDAGrAHWAGuANcAaYA30Ug1QAFEAUgCzBlgDrAHWAGug52qgb3NzU/hgDFgDrAHWAGuANcAa6KUaoACiAKQAZg2wBlgDrAHWQM/VQN8vv/wifDAGrAHWAGuANcAaYA30Ug1QAFEAUgCzBlgDrAHWAGug52qgYwF06PgZKcsjS/EufvudlOVBjvHnkjmMP4dmPOnlPGZx1zpelvF9N3ho5Sik3b63b99KJ4+yiB/DI4v/bhTVbtkgx/jfPJnD+HNorvdezmMWd63juzX+lsGOVo5C2qUAyiGAJOI/e+FlFZHFI6a4NUtXVo42R2XlZ2qPHN9uxaDM12JWDWsdt3UXc8xD+25jpJWjkHYpgCiAagNv6IsoZP/oArV4SB9C9m39zxoILB7Sh9B9Ww7k+F3oUAftH+UxK79ax62/QYMSeec2Rlo5CmmXAogCiAKo4AMUGoAsXnAabd2zHLIGO4u37aTgoOXQyxyzuGsdtzkpeOmoumdjpJWjkHYpgCiAKIBUhxdsHA1AFsc9FbeF5ZA12Fm8uAywZ5ZDL3PM4q513OYEZ693W9gYaeUopF0KIAogCqCCj21oALJ4wWm0dc9yyBrsLN62k4KDlkMvc8zirnXc5qTgpaPqno2RVo5C2u3rNLJlugssKwY28Vl4DMcRB4STo34EUI4Qrs8Ae4A4IBxb0G+BOCBcnwH2IDYOsfmLM+C/RZljRAF0/ExmxZQh8YgDwjODUyAAcUB4gaikuoL8R3hqpwU7iDggvGB0Ut1BHBCe2mnBDsbGITZ/NdJd5hhRAFEAleYrsKzBIfYLGPmP8Ky4FOk44oDwInHJ8gVxQHhWv0U6HhuH2PzVyHWZY0QBRAFEAaQxqjjYRAMQwh1MqTVFHBCu5riDYcQB4Q6m1JrGxiE2fzUSW+YYUQBRAFEAaYwqDjbRAIRwB1NqTREHhKs57mAYcUC4gym1prFxiM1fjcSWOUYUQBRAFEAao4qDTTQAIdzBlFpTxAHhao47GEYcEO5gSq1pbBxi81cjsWWOEQUQBRAFkMao4mATDUAIdzCl1hRxQLia4w6GEQeEO5hSaxobh9j81UhsmWNEAUQBRAGkMao42EQDEMIdTKk1RRwQrua4g2HEAeEOptSaxsYhNn81ElvmGFEAUQBRAGmMKg420QCEcAdTak0RB4SrOe5gGHFAuIMptaaxcYjNX43EljlGhRVAn/zhL/Lnf/4oy+siP/zjjITceDGrqEIlfvnhbZk4cVTeGapI/2BFBvYdlSOTt+XRapYnnR9HHBDeqeWXdz+XsUMjMjBY57inckomZp/I8manPWafhzggPLtnF+Qnmf20Wstn/+Siy4mwLfIf4dBAVoMfb8pokj9Tp62P0ZkXWWc6H0ccEO5ssPWE18/kzvVJOTJcr1mf3KwpxAHhtp+8zwuTO3PWmkPzeuJ+3h5xO98csMXuWgT3d/OFLMxMy9gHh2WPuYbyjg0Pp2Vv7Zo7Kdd+3ObY6zm9/qe/ykenLmwHpMv/iiWAJj6XL+Yey79fvWmiVSYBtHTrbL2wh0Zk9MyUnLs0JeMfJG+clSlZ8CyC0AWO8KZE5HqxIg+unKgJn4F9J2X8kuF4Ucber3McOH5TXubqJ38jxAHh+S1lt1ybn9wSe7kHuezumhDkP8KbOnN5sTiV5PGo7P/NzsfHt35y6a1tW8QB4W07B+DL+YuyP/kwsqdyQsbOT8n0/RVwljuMOCDc1eLSnfr4YsaYtMfYsBFIx2R6ybXn7Pa+OWRb8oOE9Hft8Ywcea8uQmsfcs14f+cZdnzzmUy/b8VrswDq9Zz+6+FT+dW7I95EUKEE0BeP1raKY/W/38sP/6u/LI0AejUnY2agrUzJ/OstqiKyIguTh2ufsIeuPGwEuv4fXeAId3bg8ed1gffhjCw1zfasyPz5EekfrHr9xGn8QxwQ7syx9YTVBZk4UJGBoZLNAN27WKvJ8fmNVsbeX6McIbxjh+wn7cpZufbQv+hp9AtxQHhjX13/v35XxocqMnD2rmyPul33Cq/F7i347SFYzO1YP3RCzt11myld+upkbZwcqInyZgHUln2P5NSnCCqUAPrjve/l27//TSbP17/ymkum/soigJZvna69oaROrz+fkWEz5fmh3xkSdIEjvO0FlwK+nDEXb0VS3zSTN9RU/il95T2EOCA8r530domwGzopN2amatzLMgNUz2VVzvn9Ri81jChHCE/tFB5MPmkPnZYb/4GNu26AOCC8awcaOli6fkz6B0fkwjcNBz38u5scPLgbSLBtyPxZ82HosHt8n8/Uvnbee+mmTH9oZoHyC6BeyqkvEVQoAdS6zqd0Auj+TG0qejZtyvn1nIwZAeT5KyI0ICHceZBZX5HlVyuy1jT7U+9lefZUTSCM3fL7SRtxQLgzx4YT1hanajNeo1+9ELlfLgH04IqZscs/ADeExflflCOEOxs0JzyclqHBihy8nuNriY4MNJ+EOCC8ubcuXm0+lAsHKtL//oykDUVd9BxIUHTjUftzg8Q8GcvdZ9deyI3jVek/cFHmV1/INRcB1IM59SGCKIAKchfY2t36GhLfgzG6wBHefvjIiW5uyNLdKTloptyP32z5aixnH22aIQ4Ib9N1e2h9Uc5VzKxd8nVfyQRQfcHlKZm4cloO7kvWcJkF+5cX5GWKwG0frPYoyhHC2/eeji59daI+C3LvidyYPCV7k/UaZsH+BcevLdItNB9FHBDe3Fvnr8xM9MBgVcbn/X4QMR7tFofO2TefGcTfZKZ7bPaJzF9tvHaOyfhM9o0gL78+KQODIzJxz+TFTQD1ak4bRdDya/d6pgAqggCyi96GJuVO9IugtweY1jsW9p6d8/7GmWfQDTLIiciDK2bd1jGZfppwLpUAsgNwRfqHRrYWQW/duehZyKIcIXy76vL/V6/PqtTWWrx3rLb4+dz503KwJoSqUq6vam1ckq/9DkzLA88i1lgIkSfreYjnEP7aZQC1NYFbN7tMymjyIWLv5OLOdVfJmqGBM3ZNlr3+8szA9nZOrQjaN/yRuIogCiB1AWQXQOt8IgsxANiBavuOBXvxV+XI9Sc7L357QofPiAPCOzL79Es5OFiRvY2L1kslgDZkafG2TF+da56x23wm18w0/WBFjszGfBeYfYMxs5ItC/Zf1xe19w+ekhuvOqqO1JNQHSI8tVPHg6Fmmq0bu8HB2vLxHMLfrQ9+rTe7mA+6ta+1WtdercidM1XpHzots1v1ZusTCyDmVMTcHm/GpN/93u0WeQogVQG0fct46qcCD1c4usAR7sGFehebL5I3zhGZuO/3riLEAeHOHO2Mndm2YL3h7FIJoAZerf8uzdTEn8/1aihHCG91Eb+2bzAn5Nrzna3tp/jRr2MWea28fpIbn5gZPf8zzdaS/zzZnsM8h/DXCqCJeyk+30+2lri0fWdBfQuNqox+3Xi3mK1PJICYUzMDtOfdEfn1b8c5A9S6kDrP65QyrR0KcXE02lqaMd/5VmTv5EKQDQKNLcQB4Y3+dv3/0y9lv9n4seHi77pPBY7121Ttd/UNDHpFAMmiTNQ2aZuShQb63fyL6hDh7rbBG0ySS5+1ijgg3J1jyxnf1Bd9D132u9VGo5XgHBqNefg/hL9bAihtg0m7uai92SXZQmNrHeEWJ1Cftl2P59SKH9eZHxs+zgApzQC9nJ+s3T205xP/i4Jtcs0zusAR3thXrv9Xs+8CE3vxfzYny7k6y9cIcUB4PitJq9dz8nGyaZ6Zcm37yLvrK3AA+Y9w0H063OZuPimFABKZP2/ylz4DZO/o87kvF8oTwtMTlfeovTXb78aHrdbDcmi11v3rEP7WF9dXJHUGyI6Bn9bHwEdX6/u/tR1HauNM2kxQb+e0W/FjqocCSEEArd2v3zod4o6o1iEBXeAIb+0PvV64ZNaHjMiFtA+ZZZgBer0o0xk7654bN3urmNuLT9d33s2z6ysKqIaINT4lu0CnLgR+dVuOGJ72U2wODqgJqkOEo/7TcLstQ9paJov53LIBcUB4Gofcx5KvLQeSN97c5zk2DMrB0Zc8zYP4225WxmLJ2sHtdZKtu3VflFGzVcHgiIyeNdiMLDRtnisiPZxTH+LH1AcF0G4LoKUZGTW3gx+algee7/hKu+DRBY7wtD7bHbML8gbGbzff8bVpd4IuwRqgrACU6SuwZFdZs2t588+z2EX7sS+CFpHV+m7IZvFp00aIq4syYbY3aFqUmpX0/MfRtYbw/JZ2tnxw2ezp1Lr4dme7bo9447C5sXM/MTO73Dhm1tp0t57Qm7+NgbNrBAePyYVvGm7N3loHeTj9A2JjHzlug48up038On/hS/wYDyiAdlMAmenP2tcnIzKc/A5Y2m/0+PwdInSBI9y9TF/I7Gf1ad203wILsdgbcUC4O8eMM8okgETEzlT221vEW37TrfmnTjJikvMwyhHCc5rZ0WyL49DRHbfBH2lalLrjVOcDiAPCnQ3aE+yMXYCND60J++yLQ31X44b1gvar56GzMlubCVmR2U/NDElVPr7TIDKsIzmfffm7w9xzO9ZXk7F++07Y/ZdSboPf0QFYAxRhTndQ7OCAT/FjzFMA7aYAsm+QYO1I6tcOHRSLOQVd4AjvzOyKLM01/hp8Vd45dFouB9hcTo9jSmRsfj2t/bEWUI4Qbvvp5Hn54W2ZOHFU7P4/tR8LvVqOjRBtPNYeG47Jr3UPVmXvB5NBfhcM5Qnh1l/X50fXzFezVRnz+OO1WT744rB856zsGWy4M2p9US4MV2VgeDq583JDFi4dlv6ho9LNmm5f/qbG4z8Lcnn8WHLtVOWdYZcNNtsLoBhzmhojx4PmdvdOFzynmSq0AMpzB5ePNmmBMceCXhxZRj0fRxwQ7tmdIN0hDggP4pTHTpH/CPfoSrCuEAeEB3PMY8eIA8I9uhKsq9g4xOZvsMS16bjMMaIA2s0ZoDZFFgpCxYvwUH757BdxQLhPX0L0hfxHeAiffPeJOCDctz8h+kMcEB7CJ999xsYhNn995ytPf2WOEQUQBVBtpivPhVDUNugCRXhReVm/kP8It/0U+RlxQHiRuVnfEAeE236K/Bwbh9j81ch9mWNEAUQBRAGkMao42EQDEMIdTKk1RRwQrua4g2HEAeEOptSaxsYhNn81ElvmGFEAUQBRAGmMKg420QCEcAdTak0RB4SrOe5gGHFAuIMptaaxcYjNX43EljlGFEAUQBRAGqOKg000ACHcwZRaU8QB4WqOOxhGHBDuYEqtaWwcYvNXI7FljhEFEAUQBZDGqOJgEw1ACHcwpdYUcUC4muMOhhEHhDuYUmsaG4fY/NVIbJljRAFEAUQBpDGqONhEAxDCHUypNUUcEK7muINhxAHhDqbUmsbGITZ/NRJb5hhRAFEAUQBpjCoONtEAhHAHU2pNEQeEqznuYBhxQLiDKbWmsXGIzV+NxJY5RhRAFEAUQBqjioNNNAAh3MGUWlPEAeFqjjsYRhwQ7mBKrWlsHGLzVyOxZY4RBRAFEAWQxqjiYBMNQAh3MKXWFHFAuJrjDoYRB4Q7mFJrGhuH2PzVSGyZY0QBRAFEAaQxqjjYRAMQwh1MqTVFHBCu5riDYcQB4Q6m1JrGxiE2fzUSW+YYUQBRAFEAaYwqDjbRAIRwB1NqTREHhKs57mAYcUC4gym1prFxiM1fjcSWOUYUQBRAFEAao4qDTTQAIdzBlFpTxAHhao47GEYcEO5gSq1pbBxi81cjsWWOEQUQBRAFkMao4mATDUAIdzCl1hRxQLia4w6GEQeEO5hSaxobh9j81UhsmWNEAZRDANkCiPk568KJmVOr72XnWHZ+Jp9Zf625jvl1L3PM4q51POY62m3ftXIU0i4FEAVQbQZoty+mEPayLpQQtjT6LDs/E9OsP414h7LZyxyzuGsdD5XjMvarlaOQdjsWQCGdYt+MACPACDACjAAjwAiEjAAFUMjosm9GgBFgBBgBRoARKGQEKIAKmRY6xQgwAowAI8AIMAIhI0ABFDK67JsRYAQYAUaAEWAEChkBCqBCpoVOMQKMACPACDACjEDICFAAhYwu+2YEGAFGgBFgBBiBQkaAAqiQaaFTjAAjwAgwAowAIxAyAhRAIaPLvhkBRoARYAQYAUagkBGgACpkWugUI8AIMAKMACPACISMAAVQyOiyb0aAEWAEGAFGgBEoZAQogAqZFjrFCDACjAAjwAgwAiEj8H+bxewSv6O+vgAAAABJRU5ErkJggg=="
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the maximum page number to do a FOR loop through\n",
    "Each page of www.jobindex.dk/jobsoegning contains 20 joblistings. Since the number of job postings differ from time to time, so does the number of pages. To be able to scrape job postings on all pages, we look at the pagination. The pagination looks as the picture below, and we want to save the number of the last page as \"last_page\" to be able to loop through it later on.  \n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "616\n"
     ]
    }
   ],
   "source": [
    "#Define the url and fetch the HTML using the requests module\n",
    "url = 'https://www.jobindex.dk/jobsoegning?lang=da.html'\n",
    "response = requests.get(url)  \n",
    "html = response.text  \n",
    "soup = BeautifulSoup(html,'lxml') # parse the raw html using BeautifoulSoup\n",
    "\n",
    "#Find the pagination and page links\n",
    "pagination = soup\\\n",
    "            .find('ul',{'class':'pagination'})\\\n",
    "            .find_all('a', {'class':'page-link'})\n",
    "#print(pagination)\n",
    "\n",
    "#Extract the text-part of the pagination links, i.e. not the link but the page number. \n",
    "page_list = []\n",
    "for page in pagination:\n",
    "    page_list.append(page.text)\n",
    "    last_page = int(page_list[-1]) #Save the last page number as \"last page\"\n",
    "print(last_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract all job postings from www.jobindex.dk. \n",
    "For each job posting one is redirected from www.jobindex.dk/jobsoegning to a separate www-webpage or PDF, that contains the full job posting. We therefore have to distinguish between these two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 0/40 of job_links\n",
      "Completed 1/40 of job_links\n",
      "Completed 2/40 of job_links\n",
      "Completed 3/40 of job_links\n",
      "Completed 4/40 of job_links\n",
      "Completed 5/40 of job_links\n",
      "Completed 6/40 of job_links\n",
      "Completed 7/40 of job_links\n",
      "Completed 8/40 of job_links\n",
      "Completed 9/40 of job_links\n",
      "Completed 10/40 of job_links\n",
      "Completed 11/40 of job_links\n",
      "Completed 12/40 of job_links\n",
      "Completed 13/40 of job_links\n",
      "Completed 14/40 of job_links\n",
      "Completed 15/40 of job_links\n",
      "Completed 16/40 of job_links\n",
      "Completed 17/40 of job_links\n",
      "Completed 18/40 of job_links\n",
      "Completed 19/40 of job_links\n",
      "Completed 20/40 of job_links\n",
      "Completed 21/40 of job_links\n",
      "Completed 22/40 of job_links\n",
      "Completed 23/40 of job_links\n",
      "Completed 24/40 of job_links\n",
      "Completed 25/40 of job_links\n",
      "Completed 26/40 of job_links\n",
      "Completed 27/40 of job_links\n",
      "Completed 28/40 of job_links\n",
      "Completed 29/40 of job_links\n",
      "Completed 30/40 of job_links\n",
      "Completed 31/40 of job_links\n",
      "Completed 32/40 of job_links\n",
      "Completed 33/40 of job_links\n",
      "Completed 34/40 of job_links\n",
      "Completed 35/40 of job_links\n",
      "Completed 36/40 of job_links\n",
      "Completed 37/40 of job_links\n",
      "Completed 38/40 of job_links\n",
      "Completed 39/40 of job_links\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2 \n",
    "import textract\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import slate3k as slate\n",
    "import pandas as pd\n",
    "\n",
    "jobindex_links = []\n",
    "for i in range(1,3): # NOTE I'm only testing it on the first two pages. Use last_page when ready\n",
    "    url = f'https://www.jobindex.dk/jobsoegning?lang=da?page={i}.html'\n",
    "    jobindex_links.append(url)   \n",
    "#print(jobindex_links)\n",
    "html = []\n",
    "job_links = []\n",
    "jobs = []\n",
    "dates = []\n",
    "\n",
    "for url in jobindex_links:\n",
    "    response = requests.get(url)  \n",
    "    html = response.text  \n",
    "    soup = BeautifulSoup(html,'lxml')\n",
    "    #one job result is given by class=jobsearch-result\n",
    "    joblistings = soup.find_all('div',{'class':'jobsearch-result'})\n",
    "    #print(joblistings)\n",
    "    for joblisting in joblistings:\n",
    "        title = re.findall(r'(?<=<b>)(.*)(?=</b>)', str(joblisting))[0]\n",
    "        links = re.findall(\"href=[\\\"\\'](.*?)[\\\"\\']\", str(joblisting))\n",
    "        link = str(links[1])\n",
    "        link = link.replace(\"&amp;\", \"&\")\n",
    "        pub_date = re.findall(r'time\\sdatetime=\"(.*)\"', str(joblisting))\n",
    "        dates.append(pub_date) \n",
    "        job = [title,link,pub_date]\n",
    "        jobs.append(job) # Jobs is a list of list where each list contains the title job, and the link for that job, this will help later\n",
    "        job_links.append(link)\n",
    "\n",
    "for i in range(len(job_links)):\n",
    "    if 'pdf' in jobs[i][1]:\n",
    "        #scraping a PDF file\n",
    "        #request the url\n",
    "        url = jobs[i][1]\n",
    "        response = requests.get(url)\n",
    "        #save the pdf in the current folder\n",
    "        with open('pdf.pdf', 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        #open the pdf and get the text in doc\n",
    "        with open('pdf.pdf', 'rb') as fp:\n",
    "            doc = slate.PDF(fp)\n",
    "            doc = str(doc).replace('\\n','')\n",
    "            doc = str(doc).replace('\\r','')\n",
    "            jobs[i].append(doc)\n",
    "    else:\n",
    "        link = jobs[i][1]\n",
    "        response = requests.get(link)\n",
    "        html = response.text\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        description = soup.get_text()\n",
    "        description = description.replace('\\n','')\n",
    "        description = description.replace('\\r','')\n",
    "        jobs[i].append(description)\n",
    "    \n",
    "    print('Completed %d/%d job_links' % (i,len(job_links)))\n",
    "\n",
    "#transforming into dataframe\n",
    "\n",
    "df = pd.DataFrame(jobs)\n",
    "df.columns = [\"Job Title\", \"Link\",\"Date\", \"Description\",]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying jobs with possibility of working remotely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we define a list of keyword that we think a job posting where working remotely is a possibility will include\n",
    "\n",
    "keywords = ['hjemmefra', 'arbejde hjemme', 'hjemmearbejde','hjemmekontor', 'arbejde hjemmefra', 'arbejde remote', 'fjernarbejde']\n",
    "\n",
    "# I will now look for these words in the description column. I am not using the tokenized version of column because then I \n",
    "# can't look for expressions with more than one word. If you want to use the tokenized version then you have to use bigrams\n",
    "\n",
    "df['Remote'] = '0'\n",
    "for word in keywords:\n",
    "    for i in range(len(df)):\n",
    "        df['Description'][i] = str(df['Description'][i]).lower()\n",
    "        if word in df['Description'][i]:\n",
    "            df['Remote'][i] = '1'\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying the most popular words in job descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first step is to clean the job descriptions as much as possible so we can narrow down the words included for \n",
    "# analysis as much as possible\n",
    "\n",
    "df['Tokenized_description'] = ' '\n",
    "symbols = ['?','!','>','<','-','[',']','(',')','{','}',' –','``',\"''\",'\"\"','\\\\','@','$','&','=']\n",
    "for i in range(len(df)):\n",
    "    stop_words = nltk.corpus.stopwords.words(\"danish\")\n",
    "    df['Tokenized_description'][i] = re.sub(r'(\\.+ )|,|\\||:|/|\\'|\\-|;|\\*|!|(\\s\\d+\\s)|(\\s\\W\\s)',' ',str(df['Description'][i]))\n",
    "    df['Tokenized_description'][i] = str(df['Tokenized_description'][i]).rstrip('\\\\')\n",
    "    df['Tokenized_description'][i] = nltk.word_tokenize(str(df['Tokenized_description'][i].lower()))\n",
    "    df['Tokenized_description'][i] = [w for w in df['Tokenized_description'][i] if not w in stop_words] \n",
    "    df['Tokenized_description'][i] = [w for w in df['Tokenized_description'][i] if not w in symbols]\n",
    "    df['Tokenized_description'][i] = [w for w in df['Tokenized_description'][i] if not w.isnumeric()]\n",
    "    df['Tokenized_description'][i] = [word for word in df['Tokenized_description'][i] if len(word) > 3]\n",
    "    \n",
    "# Finding intersections --- USELESS\n",
    "df['description_set'] = df['Tokenized_description'].apply(set)\n",
    "description_set = df['description_set'].tolist() #convert each description into a set \n",
    "set.intersection(*description_set)\n",
    "del df['description_set']\n",
    "\n",
    "# Finding most frequent words in all descriptions --- USELESS ATM BUT MAYBE IF WE CLEAN IT ENOUGH IT WILL WORK\n",
    "\n",
    "descriptions_list = [] \n",
    "# This loop will pull all tokens in one bag\n",
    "for i in range(len(df)):\n",
    "    descriptions_list.extend(df['Tokenized_description'][i]) \n",
    "    \n",
    "word_dist = nltk.FreqDist(descriptions_list) word_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL DEDICATED TO KEEP CLEANING DESCRIPTIONS\n",
    "\n",
    "def clean(doc):\n",
    "    #doc = doc.replace('\\n','')\n",
    "    #doc = doc.replace('\\r','')\n",
    "    doc = doc.replace('\\t','')\n",
    "    doc = doc.replace('\\'','')\n",
    "    doc = doc.replace('|','')\n",
    "    doc = doc.replace('/','')\n",
    "\n",
    "def strip_html(row):\n",
    "    return str(html.fromstring(row).text_content())\n",
    "\n",
    "from lxml import html\n",
    "for row in df['Description']:\n",
    "    strip_html(str(row))\n",
    "    \n",
    "    \n",
    "    \n",
    "#text = re.sub('<[^>]*>', '', text)\n",
    "#emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assigning DISCO-08 codes to the job postings\n",
    "This function will be used to assign a DISCO-08 code to the job postings. It goes through the following steps:\n",
    "\n",
    "1. Importing a csv file downloaded from Danmarks Statistik which includes the different job functions that fall under each DISCO-08 category\n",
    "2. Clean this csv file and converting it into a dataframe with two columns, one that has the disco codes and one that has a list of all the jobs that fall into each code\n",
    "3. Tokenizing the jobs \n",
    "4. Tokenizing the words in the 'Job Title' column from our job postings data\n",
    "5. Creating a function that compares the tokens in the job titles to the tokens in the DISCO dataframe and if a match is found, then returning the corresponding DISCO code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lisbe\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "# Importing and cleaning DISCO-08 classification\n",
    "import pandas as pd\n",
    "import re\n",
    "disco = pd.read_csv(r\"C:\\Users\\lisbe\\OneDrive\\11. Semester\\Social Data Science\\Modules\\csv_da.csv\",header=None)\n",
    "disco_clean = disco.copy()\n",
    "\n",
    "# Remove words that start with lowercase\n",
    "for i in range(len(disco_clean)):\n",
    "    disco_clean[3][i] = ' '.join([word for word in str(disco_clean[3][i]).split(' ') if not word.islower()])\n",
    "\n",
    "# Create a dictionary for DISCO functions\n",
    "\n",
    "disco_clean['DISCO'] = disco_clean[0].astype(str).str[:1] #This column will have the highest hierarchy code 0-9\n",
    "del disco_clean[0]\n",
    "del disco_clean[1]\n",
    "del disco_clean[2]\n",
    "disco_clean = disco_clean.groupby(by=disco_clean['DISCO']).sum()\n",
    "disco_clean.rename({3:'functions'}, axis='columns', inplace = True) # rename column \n",
    "for i in range(9): # Adding a space in between words that are missing them \"LikeThis\"\n",
    "    disco_clean['DISCO'] = disco_clean['DISCO'].astype(str).str[:1] # We only want the first number \n",
    "    disco_clean['functions'][i] = re.sub(r\"(\\w)([A-Z])\", r\"\\1 \\2\", str(disco_clean['functions'][i]))\n",
    "    \n",
    "disco_clean[\"functions_tokenized\"] = \"\"\n",
    "for i in range(len(disco_clean)):\n",
    "    disco_clean['functions_tokenized'][i]=nltk.word_tokenize(str(disco_clean['functions'][i]).lower())\n",
    "    \n",
    "# Now I would like a job type column from the job titles where I would extract the nouns from job titles\n",
    "#pip install afinn\n",
    "import nltk\n",
    "df['Job Title']=df['Job Title'].str.replace(',','')\n",
    "df[\"tokenized_titles\"] = \"\"\n",
    "for i in range(len(df)):\n",
    "    df['tokenized_titles'][i] = nltk.word_tokenize(str(df['Job Title'][i]).lower())\n",
    "\n",
    "# This loop creates a new column in the df that assigns a disco code to each posting based on the words in the title\n",
    "\n",
    "df['disco'] = ''\n",
    "for m in range(len(df)):\n",
    "    for i in range(len(disco_clean)):\n",
    "        for element in disco_clean['functions_tokenized'][i]:\n",
    "            if element in df['tokenized_titles'][m]:\n",
    "                df['disco'][m] = disco_clean['DISCO'][i]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
