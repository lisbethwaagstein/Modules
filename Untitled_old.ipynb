{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = [] # Run this one if you are going to run the loop in the next cell from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "quarters = [['0101','0331'],['0401','0630'],['0701','0930'],['1001','1231']]\n",
    "years = ['08','09','10','11','12','13','14','15','16','17','18','19','20']\n",
    "\n",
    "#Define the 'quarter 'url and find the max number of pages for that quarter\n",
    "\n",
    "# Loop through each year\n",
    "for y in years: \n",
    "    # Loop through each quarter\n",
    "    for q in quarters:\n",
    "        # define quarterly search link\n",
    "        url = f'https://www.jobindex.dk/jobsoegning?maxdate=20{y}{q[1]}&mindate=20{y}{q[0]}&jobage=archive'\n",
    "        response = requests.get(url)   \n",
    "        soup = BeautifulSoup(response.text,'lxml') # parse the raw html using BeautifoulSoup\n",
    "        # identify total number of job postings in the given quarter\n",
    "        pages = BeautifulSoup(str(soup.find_all(\"div\", {'class':'jix_pagination_total'}))).get_text() \n",
    "        total = re.findall(r'af(?s)(.*)resultater',str(pages))\n",
    "        total = str(total).strip(\"[]\").strip(\"''\").strip().replace('.','')\n",
    "        # use total number of job postings to calculate total number of pages\n",
    "        last_page = math.ceil(int(total)/20)+1\n",
    "        jobindex_links = []\n",
    "        for i in range(1,last_page):\n",
    "            url = f'https://www.jobindex.dk/jobsoegning?maxdate=20{y}{q[1]}&mindate=20{y}{q[0]}&page={i}&jobage=archive'\n",
    "            jobindex_links.append(url)    \n",
    "        for url in jobindex_links:\n",
    "            url_index = jobindex_links.index(url)\n",
    "            print('scraping url',url_index,'out of',len(jobindex_links))\n",
    "            response = requests.get(url,verify=False)\n",
    "            html = response.text    \n",
    "            if '<strong>' in html: \n",
    "                html = html.replace('<strong>','<b>')\\\n",
    "                        .replace('</strong>','</b>')\n",
    "            soup = BeautifulSoup(html,'lxml') \n",
    "            #one job result is given by class=jobsearch-result\n",
    "            joblistings = soup.find_all('div',{'class':'jobsearch-result'})\n",
    "            for joblisting in joblistings:\n",
    "                title = re.findall(r'(?<=<b>)(.*)(?=</b>)', str(joblisting))[0]\n",
    "                if len(re.findall(r'(?<=<b>)(.*)(?=</b>)', str(joblisting)))>1:\n",
    "                    company = re.findall(r'(?<=<b>)(.*)(?=</b>)', str(joblisting))[1]\n",
    "                    if 'amp;' in company:\n",
    "                                company = company.replace('amp;','') \n",
    "                else:\n",
    "                    company = ''\n",
    "                pub_date = re.findall(r'time\\sdatetime=\"(.*)\"', str(joblisting))\n",
    "                descrip = BeautifulSoup(str(joblisting), 'lxml')\n",
    "                for div in descrip.find_all(\"div\", {'class':'jix_toolbar jix_appetizer_toolbar'}): # remove toolbar at the end of each job\n",
    "                    div.decompose()\n",
    "                for span in descrip.find_all(\"span\", {'class':'jix_toolbar jix_appetizer_toolbar'}): # remove toolbar at the end of each job\n",
    "                    span.decompose()\n",
    "                links = re.findall(\"href=[\\\"\\'](.*?)[\\\"\\']\", str(descrip))\n",
    "                if len(links)>1:\n",
    "                    link = str(links[1])\n",
    "                else:\n",
    "                    link = str(links[0])\n",
    "                link = link.replace(\"&amp;\", \"&\")\n",
    "                # Indirectly extracting the short job description by removing everything else\n",
    "                descrip = descrip.get_text()\n",
    "                descrip = descrip.replace(\"\\n\", \" \") # remove \\n \n",
    "                job = [title,link,pub_date,descrip,company]\n",
    "                jobs.append(job)\n",
    "                time.sleep(2)\n",
    "        print('Finished',q,'of year 20',y)        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
