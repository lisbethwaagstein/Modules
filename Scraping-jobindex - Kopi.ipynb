{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exam: Scraping job postings from Jobindex\n",
    "\n",
    "...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scraping_class\n",
    "logfile = 'log_jobindex_scraping.txt'\n",
    "connector = scraping_class.Connector(logfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "\n",
    "#define url and fetch the HTML using the requests module\n",
    "url = 'https://www.jobindex.dk/jobsoegning?lang=da.html'\n",
    "response = requests.get(url)  \n",
    "html = response.text  \n",
    "soup = BeautifulSoup(html,'lxml') # parse the raw html using BeautifoulSoup\n",
    "#print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "648\n"
     ]
    }
   ],
   "source": [
    "#find the max page number\n",
    "max_page = soup.find('ul',{'class':'pagination'})\n",
    "max_page2 = max_page.find_all('a', {'class':'page-link'})\n",
    "\n",
    "def convert_value_type(value_node):\n",
    "    if value_node.name == 'a':\n",
    "        return value_node.text\n",
    "\n",
    "page_list = []\n",
    "for page in max_page2:\n",
    "    page_list.append(convert_value_type(page))\n",
    "#print(page_list[-1])\n",
    "\n",
    "last_page = int(page_list[-1]) + 1 # we add one to use it in range in for-loop\n",
    "print(last_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "https://stormgroup.recruitee.com/o/salgskonsulent-til-sjov-social-hverdag\" data-click=\"/c?t=h989946&amp;ctx=w&amp;u=20313204\" rel=\"noopener\" target=\"_blank\"><b>salgskonsulent til sjov, social hverdag</b></a>\n",
      "<p>\n",
      "<a \n"
     ]
    }
   ],
   "source": [
    "#check exercise 6.1.4??\n",
    "#Get all link pages from jobindex\n",
    "jobindex_links = []\n",
    "for i in range(1,last_page):\n",
    "    url = f'https://www.jobindex.dk/jobsoegning?lang=da?page={i}.html'\n",
    "    jobindex_links.append(url)   \n",
    "#print(jobindex_links)\n",
    "\n",
    "#Extract data from one page link first. We want the\n",
    "url = 'https://www.jobindex.dk/jobsoegning?lang=da?page=1.html'\n",
    "response = requests.get(url)  \n",
    "html = response.text  \n",
    "soup = BeautifulSoup(html,'lxml')\n",
    "\n",
    "#one job result is given by class=jobsearch-result\n",
    "joblistings = soup.find_all('div',{'class':'jobsearch-result'})\n",
    "#print(joblistings)\n",
    "\n",
    "# now find the a href hyperlinks of the individual jos postings. note: postings have many hyperlinks.\n",
    "#first divide into postings in a list\n",
    "postings = []\n",
    "for posting_loc in html.split('<div class=\"jobsearch-result\">')[1:]:\n",
    "    posting = posting_loc.split('</div>')[0].lower()\n",
    "    postings.append(posting)\n",
    "#print(postings)\n",
    "\n",
    "\n",
    "#now find all hyperlinks within a posting\n",
    "links = []\n",
    "for posting in postings:\n",
    "    link_loc = tuple(posting.split('href=\"')[1:])    #this is made into a tuple to be able to call the second element.\n",
    "    #link_loc2 = link_loc.split('\"')[0]\n",
    "    links.append(link_loc) \n",
    "print(len(links)) # there is 20 postings per page\n",
    "#print(links)\n",
    "\n",
    "#want to get the second element in all the \"links\" such as:\n",
    "#print(links[1][1])\n",
    "#print(links[2][1])\n",
    "print(links[14][1])\n",
    "\n",
    "all_links = []\n",
    "for i in range(1,len(links)):\n",
    "    #for link in links[i][1]:\n",
    "    print(links[i][1].split('\"')[0])\n",
    "        \n",
    "   # all_links.add(split('\"')[0]) # this is where the link to the job posting is\n",
    "\n",
    "#we have to replace \"amp\" in all links with \"\"\n",
    "   \n",
    "'''    \n",
    "    for link in link_loc:\n",
    "    second_link = link[.split('\"')[0]\n",
    "    links.append(second_link)\n",
    "print(links)\n",
    "#for link in links:\n",
    "#    link_done = link.split('\"')[0]\n",
    "#    all_links.append(link_done)\n",
    "#print(all_links) \n",
    " #   link = link.split('\"')[0]\n",
    "'''\n",
    "'''for link_loc in postings_join.split('href=\"')[1:]:\n",
    "    link = link_loc.split('\"')[0]\n",
    "    links.add(link)\n",
    "print(links)'''\n",
    "\n",
    "'''for link_loc in \n",
    "    link = link_loc.split('\"')[0]\n",
    "    if '/categories/' in link:\n",
    "        links.add(link)\n",
    "print(len(links),list(links)[0]) # link is relative\n",
    "links = ['https://www.trustpilot.com'+link for link in links]# add the domain to each link\n",
    "links[:10]\n",
    "'''\n",
    "'''\n",
    "url = 'https://www.jobindex.dk/jobsoegning?lang=da?page=1.html'\n",
    "response, call_id = connector.get(url,'mapping_categories')\n",
    "if response.ok:\n",
    "    html = response.text\n",
    "else:\n",
    "    print('error')\n",
    "'''                                      \n",
    "'''\n",
    "#Extract data from all the links\n",
    "jobindex_data = []\n",
    "for link in jobindex_links:\n",
    "    response = requests.get(link)  \n",
    "    html = response.text  \n",
    "    soup = BeautifulSoup(html,'lxml') # parse the raw html using BeautifoulSoup\n",
    "    \n",
    "    #print(soup) '''   \n",
    "https://candidate.hr-manager.net/ApplicationInit.aspx?cid=1627&ProjectId=143600&DepartmentId=18956&MediaId=4619"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check exercise 6.1.4??\n",
    "#Get all link pages from jobindex\n",
    "jobindex_links = []\n",
    "for i in range(1,last_page):\n",
    "    url = f'https://www.jobindex.dk/jobsoegning?lang=da?page={i}.html'\n",
    "    jobindex_links.append(url)   \n",
    "#print(jobindex_links)\n",
    "\n",
    "#Extract data from one page link first. We want the\n",
    "url = 'https://www.jobindex.dk/jobsoegning?lang=da?page=1.html'\n",
    "response = requests.get(url)  \n",
    "html = response.text  \n",
    "soup = BeautifulSoup(html,'lxml')\n",
    "\n",
    "#one job result is given by class=jobsearch-result\n",
    "joblistings = soup.find_all('div',{'class':'jobsearch-result'})\n",
    "#print(joblistings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "for joblisting in joblistings:\n",
    "    links = re.findall(\"href=[\\\"\\'](.*?)[\\\"\\']\", str(joblisting))\n",
    "    link = links[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://frederikshavn.emply.net/recruitment/vacancyAd.aspx?publishingId=004cab7a-e5de-4811-b886-db43e9eeedc1'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
