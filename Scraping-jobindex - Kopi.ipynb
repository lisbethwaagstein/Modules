{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exam: Scraping job postings from Jobindex\n",
    "\n",
    "...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scraping_class\n",
    "logfile = 'log_jobindex_scraping.txt'\n",
    "connector = scraping_class.Connector(logfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "\n",
    "#define url and fetch the HTML using the requests module\n",
    "url = 'https://www.jobindex.dk/jobsoegning?lang=da.html'\n",
    "response = requests.get(url)  \n",
    "html = response.text  \n",
    "soup = BeautifulSoup(html,'lxml') # parse the raw html using BeautifoulSoup\n",
    "#print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "649\n"
     ]
    }
   ],
   "source": [
    "#find the max page number\n",
    "max_page = soup.find('ul',{'class':'pagination'})\n",
    "max_page2 = max_page.find_all('a', {'class':'page-link'})\n",
    "\n",
    "def convert_value_type(value_node):\n",
    "    if value_node.name == 'a':\n",
    "        return value_node.text\n",
    "\n",
    "page_list = []\n",
    "for page in max_page2:\n",
    "    page_list.append(convert_value_type(page))\n",
    "#print(page_list[-1])\n",
    "\n",
    "last_page = int(page_list[-1]) + 1 # we add one to use it in range in for-loop\n",
    "print(last_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "https://stormgroup.recruitee.com/o/salgskonsulent-til-sjov-social-hverdag\" data-click=\"/c?t=h989946&amp;ctx=w&amp;u=20313204\" rel=\"noopener\" target=\"_blank\"><b>salgskonsulent til sjov, social hverdag</b></a>\n",
      "<p>\n",
      "<a \n",
      "https://ikast-brande.emply.net/recruitment/vacancyad.aspx?publishingid=1fb545a3-1946-4afc-8484-9c336901cb5b\n",
      "https://ikast-brande.emply.net/recruitment/vacancyad.aspx?publishingid=5346be46-89e6-4202-aa7e-5f497d5ada6b\n",
      "https://www.jobindex.dk/jobannonce/366043/socialfaglig-medarbejder\n",
      "https://www.birn-partners.com/jobs/?hr=show-job%2f59547&amp;locale=da_dk\n",
      "https://www.jobindex.dk/jobannonce/366045/udekoerende-daekmontoer-til-alsidigt-arbejde\n",
      "https://www.innomate.com/innomatepublicpagesmedarb/jobnotice.aspx?companyid=zbc&amp;jobnoticeid=477\n",
      "https://www.innomate.com/innomatepublicpagesmedarb/jobnotice.aspx?companyid=zbc&amp;jobnoticeid=479\n",
      "https://candidate.hr-manager.net/applicationinit.aspx?cid=5001&amp;projectid=134384&amp;departmentid=7957&amp;mediaid=5&amp;skipadvertisement=false\n",
      "https://www.odense.dk/job?vacancyid=27322\n",
      "https://www.innomate.com/innomatepublicpagesmedarb/jobnotice.aspx?companyid=sde&amp;jobnoticeid=1176\n",
      "https://candidate.hr-manager.net/applicationinit.aspx?cid=1627&amp;projectid=143600&amp;departmentid=18956&amp;mediaid=4619\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-242-af90b78b6159>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;31m#for link in links[i][1]:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\"'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m    \u001b[1;31m# all_links.add(split('\"')[0]) # this is where the link to the job posting is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "#check exercise 6.1.4??\n",
    "#Get all link pages from jobindex\n",
    "jobindex_links = []\n",
    "for i in range(1,last_page):\n",
    "    url = f'https://www.jobindex.dk/jobsoegning?lang=da?page={i}.html'\n",
    "    jobindex_links.append(url)   \n",
    "#print(jobindex_links)\n",
    "\n",
    "#Extract data from one page link first. We want the\n",
    "url = 'https://www.jobindex.dk/jobsoegning?lang=da?page=1.html'\n",
    "response = requests.get(url)  \n",
    "html = response.text  \n",
    "soup = BeautifulSoup(html,'lxml')\n",
    "\n",
    "#one job result is given by class=jobsearch-result\n",
    "joblistings = soup.find_all('div',{'class':'jobsearch-result'})\n",
    "#print(joblistings)\n",
    "\n",
    "# now find the a href hyperlinks of the individual jos postings. note: postings have many hyperlinks.\n",
    "#first divide into postings in a list\n",
    "postings = []\n",
    "for posting_loc in html.split('<div class=\"jobsearch-result\">')[1:]:\n",
    "    posting = posting_loc.split('</div>')[0].lower()\n",
    "    postings.append(posting)\n",
    "#print(postings)\n",
    "\n",
    "#now find all hyperlinks within a posting\n",
    "links = []\n",
    "for posting in postings:\n",
    "    link_loc = tuple(posting.split('href=\"')[1:])    #this is made into a tuple to be able to call the second element.\n",
    "    #link_loc2 = link_loc.split('\"')[0]\n",
    "    links.append(link_loc) \n",
    "print(len(links)) # there is 20 postings per page\n",
    "#print(links)\n",
    "\n",
    "#want to get the second element in all the \"links\" such as:\n",
    "#print(links[1][1])\n",
    "#print(links[2][1])\n",
    "print(links[14][1])\n",
    "\n",
    "all_links = []\n",
    "for i in range(1,len(links)):\n",
    "    #for link in links[i][1]:\n",
    "    print(links[i][1].split('\"')[0])\n",
    "        \n",
    "   # all_links.add(split('\"')[0]) # this is where the link to the job posting is\n",
    "\n",
    "#we have to replace \"amp\" in all links with \"\"\n",
    "   \n",
    "'''    \n",
    "    for link in link_loc:\n",
    "    second_link = link[.split('\"')[0]\n",
    "    links.append(second_link)\n",
    "print(links)\n",
    "#for link in links:\n",
    "#    link_done = link.split('\"')[0]\n",
    "#    all_links.append(link_done)\n",
    "#print(all_links) \n",
    " #   link = link.split('\"')[0]\n",
    "'''\n",
    "'''for link_loc in postings_join.split('href=\"')[1:]:\n",
    "    link = link_loc.split('\"')[0]\n",
    "    links.add(link)\n",
    "print(links)'''\n",
    "\n",
    "'''for link_loc in \n",
    "    link = link_loc.split('\"')[0]\n",
    "    if '/categories/' in link:\n",
    "        links.add(link)\n",
    "print(len(links),list(links)[0]) # link is relative\n",
    "links = ['https://www.trustpilot.com'+link for link in links]# add the domain to each link\n",
    "links[:10]\n",
    "'''\n",
    "'''\n",
    "url = 'https://www.jobindex.dk/jobsoegning?lang=da?page=1.html'\n",
    "response, call_id = connector.get(url,'mapping_categories')\n",
    "if response.ok:\n",
    "    html = response.text\n",
    "else:\n",
    "    print('error')\n",
    "'''                                      \n",
    "'''\n",
    "#Extract data from all the links\n",
    "jobindex_data = []\n",
    "for link in jobindex_links:\n",
    "    response = requests.get(link)  \n",
    "    html = response.text  \n",
    "    soup = BeautifulSoup(html,'lxml') # parse the raw html using BeautifoulSoup\n",
    "    \n",
    "    #print(soup) '''   \n",
    "https://candidate.hr-manager.net/ApplicationInit.aspx?cid=1627&ProjectId=143600&DepartmentId=18956&MediaId=4619"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\miche\\anaconda3\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied: click in c:\\users\\miche\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\miche\\anaconda3\\lib\\site-packages (from nltk) (4.47.0)\n",
      "Requirement already satisfied: regex in c:\\users\\miche\\anaconda3\\lib\\site-packages (from nltk) (2020.6.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\miche\\anaconda3\\lib\\site-packages (from nltk) (0.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install PyPDF2\n",
    "#pip install textract\n",
    "#pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first save the file in the script folder\n",
    "import requests\n",
    "url = 'https://www.jobindex.dk/img/pdf/LRS210820SOC.pdf'\n",
    "response = requests.get(url)\n",
    "\n",
    "with open('pdf.pdf', 'wb') as f:\n",
    "    f.write(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Er du uddannet revisor og søger et job, hvor du stadig har tid til familie og fritidsaktiviteter?  Har du lyst til at yde rådgivning og service til fremtidens landmænd samt mindre og mellemstore virksomheder inden for andre erhverv? Så se lige her. Landbrugsrådgivning Syd tilbyder et job som revisor, hvor du arbejder 37 timer om ugen i gennemsnit.  Vi forventer, at du lægger 100 overarbejdstimer i 1. halvår, som du så afspadserer 1:1,25 i 2. halvår.  Vi søger en revisor til vores kontor i Varde, som kan yde skatte- og regnskabsmæssig rådgivning til alle vores kunder. Du får omkring 100 nye kolleger, hvoraf de 15 sidder på kontoret i Varde.  Vi søger en revisor, der:  har erfaring fra tilsvarende job med at udarbejde skatteregnskaber og årsrapporter  er uddannet inden for skat, regnskab og revision    har et godt overblik  har flair for kundekontakt og kan formidle budskaber i et letforståeligt sprog  kan udfordre kunderne på deres resultater og planer  kan arbejde selvstændigt og struktureret, men også samarbejde med andre Hvad får du hos Landbrugsrådgivning Syd?  Et spændende job med daglige udfordringer  Ansvar for egen kundeportefølje bestående af landbrug og andre erhverv  En arbejdsplads med en uformel omgangstone og flad organisationsstruktur  Mulighed for efteruddannelse og hjemmearbejde  Fleksible arbejdstider  Ansættelse på fuld tid Yderligere information kan fås hos revisor Claus Olsen eller chefkonsulent Helle Rønning på tlf. 7374 2020.  Send din ansøgning og dit CV mærket revisor til her@lrs.dk eller Landbrugsrådgivning Syd, Rådhusstræde 2, 6240 Løgumkloster snarest muligt. Vi gennemgår ansøgninger og afholder samtaler løbende. Se mere om os på lrs.dk \n"
     ]
    }
   ],
   "source": [
    "import PyPDF2 \n",
    "import textract\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#scraping a PDF file\n",
    "filename = 'pdf.pdf' \n",
    "\n",
    "#open allows you to read the file.\n",
    "pdfFileObj = open(filename,'rb')\n",
    "\n",
    "#The pdfReader variable is a readable object that will be parsed.\n",
    "pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "\n",
    "#Discerning the number of pages will allow us to parse through all the pages.\n",
    "num_pages = pdfReader.numPages\n",
    "count = 0\n",
    "text = \"\"\n",
    "\n",
    "#while loop reads each page and extracts the text\n",
    "while count < num_pages:\n",
    "    pageObj = pdfReader.getPage(count)\n",
    "    count +=1\n",
    "    text += pageObj.extractText()\n",
    "    pdf_text = str(text).replace('\\n','')\n",
    "print(pdf_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
